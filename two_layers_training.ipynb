{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from The_Payne import training\n",
    "from The_Payne import utils\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra =np.load('reduced_spectra_final.npy')\n",
    "labels =np.load('full_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 100\n",
    "upper_bound = 1200\n",
    "neuron_step_size = 200\n",
    "steps = 30000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Label Dimensions (15379, 5)\n",
      "Validation Labels Dimensions (3845, 5)\n",
      "Training Spectra Dimensions (15379, 10001)\n",
      "Validation Spectra Dimensions (3845, 10001)\n",
      "Neuron Range:  range(100, 1200, 200)\n",
      "======================Linear Model with 2 hidden layers=====================\n",
      "====================== Number of Neurons:  100 ============================\n",
      "nsamples:  15379\n",
      "nbatches:  30\n",
      "nsamples_valid:  3845\n",
      "nbatches_valid:  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Allen\\Desktop\\Code For Final Project\\two layers\\The_Payne\\radam.py:48: UserWarning: This overload of addcmul_ is deprecated:\n",
      "\taddcmul_(Number value, Tensor tensor1, Tensor tensor2)\n",
      "Consider using one of the following signatures instead:\n",
      "\taddcmul_(Tensor tensor1, Tensor tensor2, *, Number value) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:882.)\n",
      "  exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0: training loss = 7803.050 validation loss = 7725.027\n",
      "iteration 100: training loss = 598.072 validation loss = 599.406\n",
      "iteration 200: training loss = 291.572 validation loss = 300.931\n",
      "iteration 300: training loss = 231.782 validation loss = 238.172\n",
      "iteration 400: training loss = 201.309 validation loss = 206.767\n",
      "iteration 500: training loss = 177.549 validation loss = 187.158\n",
      "iteration 600: training loss = 163.063 validation loss = 174.045\n",
      "iteration 700: training loss = 164.997 validation loss = 162.451\n",
      "iteration 800: training loss = 152.477 validation loss = 154.834\n",
      "iteration 900: training loss = 134.707 validation loss = 147.646\n",
      "iteration 1000: training loss = 135.441 validation loss = 141.375\n",
      "iteration 1100: training loss = 141.338 validation loss = 137.707\n",
      "iteration 1200: training loss = 130.862 validation loss = 133.470\n",
      "iteration 1300: training loss = 124.408 validation loss = 130.370\n",
      "iteration 1400: training loss = 122.058 validation loss = 127.013\n",
      "iteration 1500: training loss = 119.429 validation loss = 124.478\n",
      "iteration 1600: training loss = 119.714 validation loss = 121.687\n",
      "iteration 1700: training loss = 111.574 validation loss = 119.228\n",
      "iteration 1800: training loss = 111.653 validation loss = 116.571\n",
      "iteration 1900: training loss = 117.593 validation loss = 115.354\n",
      "iteration 2000: training loss = 111.452 validation loss = 113.702\n",
      "iteration 2100: training loss = 109.627 validation loss = 112.023\n",
      "iteration 2200: training loss = 110.896 validation loss = 110.586\n",
      "iteration 2300: training loss = 104.750 validation loss = 109.873\n",
      "iteration 2400: training loss = 103.485 validation loss = 108.263\n",
      "iteration 2500: training loss = 103.608 validation loss = 107.838\n",
      "iteration 2600: training loss = 103.132 validation loss = 107.230\n",
      "iteration 2700: training loss = 107.578 validation loss = 105.205\n",
      "iteration 2800: training loss = 102.464 validation loss = 104.462\n",
      "iteration 2900: training loss = 98.776 validation loss = 104.197\n",
      "iteration 3000: training loss = 100.649 validation loss = 103.482\n",
      "iteration 3100: training loss = 101.474 validation loss = 101.588\n",
      "iteration 3200: training loss = 100.220 validation loss = 101.877\n",
      "iteration 3300: training loss = 92.924 validation loss = 101.295\n",
      "iteration 3400: training loss = 99.135 validation loss = 100.266\n",
      "iteration 3500: training loss = 95.510 validation loss = 100.665\n",
      "iteration 3600: training loss = 104.364 validation loss = 99.425\n",
      "iteration 3700: training loss = 98.122 validation loss = 98.893\n",
      "iteration 3800: training loss = 93.046 validation loss = 98.996\n",
      "iteration 3900: training loss = 91.534 validation loss = 98.103\n",
      "iteration 4000: training loss = 95.645 validation loss = 96.857\n",
      "iteration 4100: training loss = 90.956 validation loss = 97.005\n",
      "iteration 4200: training loss = 97.824 validation loss = 96.605\n",
      "iteration 4300: training loss = 91.668 validation loss = 96.921\n",
      "iteration 4400: training loss = 93.611 validation loss = 95.912\n",
      "iteration 4500: training loss = 89.808 validation loss = 95.675\n",
      "iteration 4600: training loss = 91.128 validation loss = 95.364\n",
      "iteration 4700: training loss = 95.844 validation loss = 94.899\n",
      "iteration 4800: training loss = 89.456 validation loss = 94.950\n",
      "iteration 4900: training loss = 93.097 validation loss = 95.039\n",
      "iteration 5000: training loss = 95.757 validation loss = 94.147\n",
      "iteration 5100: training loss = 89.829 validation loss = 94.070\n",
      "iteration 5200: training loss = 92.969 validation loss = 93.041\n",
      "iteration 5300: training loss = 92.109 validation loss = 92.794\n",
      "iteration 5400: training loss = 89.706 validation loss = 93.169\n",
      "iteration 5500: training loss = 88.308 validation loss = 92.999\n",
      "iteration 5600: training loss = 84.795 validation loss = 92.306\n",
      "iteration 5700: training loss = 88.453 validation loss = 91.940\n",
      "iteration 5800: training loss = 91.293 validation loss = 91.925\n",
      "iteration 5900: training loss = 90.856 validation loss = 92.099\n",
      "iteration 6000: training loss = 89.022 validation loss = 92.012\n",
      "iteration 6100: training loss = 88.726 validation loss = 91.317\n",
      "iteration 6200: training loss = 86.256 validation loss = 91.795\n",
      "iteration 6300: training loss = 86.845 validation loss = 90.833\n",
      "iteration 6400: training loss = 89.928 validation loss = 90.473\n",
      "iteration 6500: training loss = 89.983 validation loss = 90.294\n",
      "iteration 6600: training loss = 86.242 validation loss = 90.537\n",
      "iteration 6700: training loss = 84.422 validation loss = 89.876\n",
      "iteration 6800: training loss = 87.814 validation loss = 90.038\n",
      "iteration 6900: training loss = 90.467 validation loss = 89.594\n",
      "iteration 7000: training loss = 85.843 validation loss = 89.076\n",
      "iteration 7100: training loss = 82.299 validation loss = 88.950\n",
      "iteration 7200: training loss = 90.474 validation loss = 89.910\n",
      "iteration 7300: training loss = 87.244 validation loss = 88.438\n",
      "iteration 7400: training loss = 89.364 validation loss = 87.975\n",
      "iteration 7500: training loss = 84.233 validation loss = 90.223\n",
      "iteration 7600: training loss = 81.101 validation loss = 87.868\n",
      "iteration 7700: training loss = 83.985 validation loss = 88.265\n",
      "iteration 7800: training loss = 86.928 validation loss = 88.393\n",
      "iteration 7900: training loss = 86.763 validation loss = 89.233\n",
      "iteration 8000: training loss = 86.233 validation loss = 87.859\n",
      "iteration 8100: training loss = 88.682 validation loss = 87.802\n",
      "iteration 8200: training loss = 84.273 validation loss = 87.972\n",
      "iteration 8300: training loss = 91.498 validation loss = 87.609\n",
      "iteration 8400: training loss = 82.464 validation loss = 87.426\n",
      "iteration 8500: training loss = 86.654 validation loss = 87.075\n",
      "iteration 8600: training loss = 83.278 validation loss = 87.765\n",
      "iteration 8700: training loss = 86.441 validation loss = 87.197\n",
      "iteration 8800: training loss = 86.374 validation loss = 86.203\n",
      "iteration 8900: training loss = 86.660 validation loss = 86.505\n",
      "iteration 9000: training loss = 83.625 validation loss = 87.129\n",
      "iteration 9100: training loss = 84.719 validation loss = 85.877\n",
      "iteration 9200: training loss = 86.720 validation loss = 86.351\n",
      "iteration 9300: training loss = 85.732 validation loss = 86.601\n",
      "iteration 9400: training loss = 87.510 validation loss = 85.255\n",
      "iteration 9500: training loss = 81.257 validation loss = 86.268\n",
      "iteration 9600: training loss = 84.765 validation loss = 85.585\n",
      "iteration 9700: training loss = 83.537 validation loss = 86.105\n",
      "iteration 9800: training loss = 83.097 validation loss = 85.830\n",
      "iteration 9900: training loss = 85.528 validation loss = 88.153\n",
      "iteration 10000: training loss = 82.170 validation loss = 85.602\n",
      "iteration 10100: training loss = 85.698 validation loss = 85.468\n",
      "iteration 10200: training loss = 86.191 validation loss = 86.070\n",
      "iteration 10300: training loss = 82.904 validation loss = 84.852\n",
      "iteration 10400: training loss = 78.130 validation loss = 85.577\n",
      "iteration 10500: training loss = 81.257 validation loss = 84.727\n",
      "iteration 10600: training loss = 79.468 validation loss = 85.113\n",
      "iteration 10700: training loss = 78.991 validation loss = 84.276\n",
      "iteration 10800: training loss = 81.882 validation loss = 85.534\n",
      "iteration 10900: training loss = 84.177 validation loss = 84.310\n",
      "iteration 11000: training loss = 77.707 validation loss = 84.113\n",
      "iteration 11100: training loss = 83.098 validation loss = 84.378\n",
      "iteration 11200: training loss = 82.807 validation loss = 84.721\n",
      "iteration 11300: training loss = 81.703 validation loss = 83.605\n",
      "iteration 11400: training loss = 81.189 validation loss = 84.232\n",
      "iteration 11500: training loss = 80.845 validation loss = 83.434\n",
      "iteration 11600: training loss = 83.079 validation loss = 84.092\n",
      "iteration 11700: training loss = 82.338 validation loss = 84.764\n",
      "iteration 11800: training loss = 78.971 validation loss = 83.558\n",
      "iteration 11900: training loss = 85.187 validation loss = 84.005\n",
      "iteration 12000: training loss = 85.011 validation loss = 83.041\n",
      "iteration 12100: training loss = 80.684 validation loss = 83.973\n",
      "iteration 12200: training loss = 83.565 validation loss = 83.538\n",
      "iteration 12300: training loss = 80.600 validation loss = 83.623\n",
      "iteration 12400: training loss = 82.424 validation loss = 83.839\n",
      "iteration 12500: training loss = 79.977 validation loss = 82.873\n",
      "iteration 12600: training loss = 80.658 validation loss = 82.703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12700: training loss = 83.832 validation loss = 82.900\n",
      "iteration 12800: training loss = 82.363 validation loss = 83.235\n",
      "iteration 12900: training loss = 80.539 validation loss = 83.693\n",
      "iteration 13000: training loss = 86.999 validation loss = 82.617\n",
      "iteration 13100: training loss = 82.199 validation loss = 83.094\n",
      "iteration 13200: training loss = 80.869 validation loss = 82.081\n",
      "iteration 13300: training loss = 79.425 validation loss = 83.315\n",
      "iteration 13400: training loss = 81.790 validation loss = 83.644\n",
      "iteration 13500: training loss = 81.555 validation loss = 82.961\n",
      "iteration 13600: training loss = 79.246 validation loss = 82.307\n",
      "iteration 13700: training loss = 76.588 validation loss = 82.520\n",
      "iteration 13800: training loss = 80.627 validation loss = 82.354\n",
      "iteration 13900: training loss = 81.243 validation loss = 81.482\n",
      "iteration 14000: training loss = 80.611 validation loss = 82.512\n",
      "iteration 14100: training loss = 81.354 validation loss = 82.334\n",
      "iteration 14200: training loss = 75.858 validation loss = 82.024\n",
      "iteration 14300: training loss = 75.334 validation loss = 81.620\n",
      "iteration 14400: training loss = 82.477 validation loss = 81.964\n",
      "iteration 14500: training loss = 81.524 validation loss = 81.503\n",
      "iteration 14600: training loss = 80.848 validation loss = 81.699\n",
      "iteration 14700: training loss = 79.024 validation loss = 81.491\n",
      "iteration 14800: training loss = 77.508 validation loss = 81.533\n",
      "iteration 14900: training loss = 83.433 validation loss = 82.126\n",
      "iteration 15000: training loss = 78.233 validation loss = 81.180\n",
      "iteration 15100: training loss = 77.503 validation loss = 81.782\n",
      "iteration 15200: training loss = 80.279 validation loss = 81.157\n",
      "iteration 15300: training loss = 80.027 validation loss = 81.122\n",
      "iteration 15400: training loss = 78.974 validation loss = 81.340\n",
      "iteration 15500: training loss = 77.856 validation loss = 81.278\n",
      "iteration 15600: training loss = 81.029 validation loss = 80.895\n",
      "iteration 15700: training loss = 81.130 validation loss = 80.876\n",
      "iteration 15800: training loss = 78.975 validation loss = 80.557\n",
      "iteration 15900: training loss = 83.073 validation loss = 81.492\n",
      "iteration 16000: training loss = 78.555 validation loss = 81.359\n",
      "iteration 16100: training loss = 75.397 validation loss = 80.584\n",
      "iteration 16200: training loss = 74.780 validation loss = 80.362\n",
      "iteration 16300: training loss = 82.032 validation loss = 80.893\n",
      "iteration 16400: training loss = 83.451 validation loss = 80.991\n",
      "iteration 16500: training loss = 79.553 validation loss = 81.340\n",
      "iteration 16600: training loss = 75.417 validation loss = 80.171\n",
      "iteration 16700: training loss = 79.765 validation loss = 81.883\n",
      "iteration 16800: training loss = 80.057 validation loss = 82.246\n",
      "iteration 16900: training loss = 77.393 validation loss = 80.502\n",
      "iteration 17000: training loss = 79.075 validation loss = 80.979\n",
      "iteration 17100: training loss = 78.165 validation loss = 80.462\n",
      "iteration 17200: training loss = 78.909 validation loss = 80.280\n",
      "iteration 17300: training loss = 82.668 validation loss = 80.625\n",
      "iteration 17400: training loss = 78.384 validation loss = 80.171\n",
      "iteration 17500: training loss = 78.753 validation loss = 79.642\n",
      "iteration 17600: training loss = 81.964 validation loss = 80.715\n",
      "iteration 17700: training loss = 81.477 validation loss = 79.910\n",
      "iteration 17800: training loss = 79.352 validation loss = 79.955\n",
      "iteration 17900: training loss = 76.513 validation loss = 80.102\n",
      "iteration 18000: training loss = 78.745 validation loss = 80.033\n",
      "iteration 18100: training loss = 75.131 validation loss = 80.268\n",
      "iteration 18200: training loss = 78.391 validation loss = 79.970\n",
      "iteration 18300: training loss = 80.995 validation loss = 79.795\n",
      "iteration 18400: training loss = 81.837 validation loss = 79.867\n",
      "iteration 18500: training loss = 78.965 validation loss = 79.594\n",
      "iteration 18600: training loss = 79.154 validation loss = 79.638\n",
      "iteration 18700: training loss = 76.197 validation loss = 79.632\n",
      "iteration 18800: training loss = 74.766 validation loss = 79.419\n",
      "iteration 18900: training loss = 73.592 validation loss = 79.812\n",
      "iteration 19000: training loss = 79.158 validation loss = 78.635\n",
      "iteration 19100: training loss = 80.363 validation loss = 79.562\n",
      "iteration 19200: training loss = 75.715 validation loss = 79.434\n",
      "iteration 19300: training loss = 76.086 validation loss = 79.494\n",
      "iteration 19400: training loss = 75.077 validation loss = 78.958\n",
      "iteration 19500: training loss = 74.366 validation loss = 80.556\n",
      "iteration 19600: training loss = 76.171 validation loss = 79.010\n",
      "iteration 19700: training loss = 82.473 validation loss = 79.474\n",
      "iteration 19800: training loss = 79.846 validation loss = 79.208\n",
      "iteration 19900: training loss = 74.916 validation loss = 78.330\n",
      "iteration 20000: training loss = 79.384 validation loss = 79.732\n",
      "iteration 20100: training loss = 75.523 validation loss = 78.925\n",
      "iteration 20200: training loss = 79.446 validation loss = 79.217\n",
      "iteration 20300: training loss = 71.735 validation loss = 78.607\n",
      "iteration 20400: training loss = 77.482 validation loss = 78.921\n",
      "iteration 20500: training loss = 74.136 validation loss = 79.309\n",
      "iteration 20600: training loss = 75.990 validation loss = 78.813\n",
      "iteration 20700: training loss = 79.125 validation loss = 79.328\n",
      "iteration 20800: training loss = 79.418 validation loss = 78.466\n",
      "iteration 20900: training loss = 79.412 validation loss = 78.524\n",
      "iteration 21000: training loss = 73.492 validation loss = 79.200\n",
      "iteration 21100: training loss = 77.036 validation loss = 79.950\n",
      "iteration 21200: training loss = 79.554 validation loss = 78.375\n",
      "iteration 21300: training loss = 76.222 validation loss = 78.680\n",
      "iteration 21400: training loss = 73.209 validation loss = 78.268\n",
      "iteration 21500: training loss = 72.138 validation loss = 78.210\n",
      "iteration 21600: training loss = 74.074 validation loss = 78.173\n",
      "iteration 21700: training loss = 75.463 validation loss = 78.152\n",
      "iteration 21800: training loss = 77.171 validation loss = 78.466\n",
      "iteration 21900: training loss = 77.601 validation loss = 79.279\n",
      "iteration 22000: training loss = 79.894 validation loss = 78.538\n",
      "iteration 22100: training loss = 73.394 validation loss = 78.347\n",
      "iteration 22200: training loss = 76.620 validation loss = 78.220\n",
      "iteration 22300: training loss = 80.742 validation loss = 78.491\n",
      "iteration 22400: training loss = 72.522 validation loss = 77.414\n",
      "iteration 22500: training loss = 73.354 validation loss = 77.693\n",
      "iteration 22600: training loss = 77.907 validation loss = 78.388\n",
      "iteration 22700: training loss = 79.324 validation loss = 77.987\n",
      "iteration 22800: training loss = 75.354 validation loss = 77.931\n",
      "iteration 22900: training loss = 73.669 validation loss = 78.582\n",
      "iteration 23000: training loss = 71.868 validation loss = 77.793\n",
      "iteration 23100: training loss = 78.021 validation loss = 78.891\n",
      "iteration 23200: training loss = 72.592 validation loss = 77.738\n",
      "iteration 23300: training loss = 76.519 validation loss = 77.520\n",
      "iteration 23400: training loss = 76.140 validation loss = 77.651\n",
      "iteration 23500: training loss = 73.365 validation loss = 77.532\n",
      "iteration 23600: training loss = 79.953 validation loss = 77.817\n",
      "iteration 23700: training loss = 75.011 validation loss = 78.175\n",
      "iteration 23800: training loss = 75.014 validation loss = 77.618\n",
      "iteration 23900: training loss = 75.640 validation loss = 77.244\n",
      "iteration 24000: training loss = 74.798 validation loss = 77.180\n",
      "iteration 24100: training loss = 75.626 validation loss = 77.959\n",
      "iteration 24200: training loss = 76.892 validation loss = 78.272\n",
      "iteration 24300: training loss = 73.188 validation loss = 77.336\n",
      "iteration 24400: training loss = 74.795 validation loss = 77.096\n",
      "iteration 24500: training loss = 74.051 validation loss = 77.345\n",
      "iteration 24600: training loss = 75.925 validation loss = 78.049\n",
      "iteration 24700: training loss = 72.566 validation loss = 77.252\n",
      "iteration 24800: training loss = 72.823 validation loss = 77.446\n",
      "iteration 24900: training loss = 77.646 validation loss = 77.641\n",
      "iteration 25000: training loss = 74.772 validation loss = 77.323\n",
      "iteration 25100: training loss = 73.853 validation loss = 77.525\n",
      "iteration 25200: training loss = 73.036 validation loss = 77.247\n",
      "iteration 25300: training loss = 73.299 validation loss = 77.597\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 25400: training loss = 78.626 validation loss = 76.541\n",
      "iteration 25500: training loss = 78.722 validation loss = 78.856\n",
      "iteration 25600: training loss = 69.893 validation loss = 76.772\n",
      "iteration 25700: training loss = 72.380 validation loss = 77.884\n",
      "iteration 25800: training loss = 76.011 validation loss = 77.223\n",
      "iteration 25900: training loss = 70.482 validation loss = 77.096\n",
      "iteration 26000: training loss = 73.113 validation loss = 76.569\n",
      "iteration 26100: training loss = 74.757 validation loss = 77.697\n",
      "iteration 26200: training loss = 74.670 validation loss = 78.903\n",
      "iteration 26300: training loss = 75.028 validation loss = 77.149\n",
      "iteration 26400: training loss = 72.892 validation loss = 77.185\n",
      "iteration 26500: training loss = 73.814 validation loss = 77.181\n",
      "iteration 26600: training loss = 74.322 validation loss = 76.405\n",
      "iteration 26700: training loss = 75.299 validation loss = 76.802\n",
      "iteration 26800: training loss = 72.566 validation loss = 76.562\n",
      "iteration 26900: training loss = 73.458 validation loss = 77.301\n",
      "iteration 27000: training loss = 73.976 validation loss = 76.970\n",
      "iteration 27100: training loss = 73.522 validation loss = 76.727\n",
      "iteration 27200: training loss = 77.340 validation loss = 76.318\n",
      "iteration 27300: training loss = 74.340 validation loss = 77.791\n",
      "iteration 27400: training loss = 76.766 validation loss = 77.102\n",
      "iteration 27500: training loss = 75.681 validation loss = 78.223\n",
      "iteration 27600: training loss = 71.012 validation loss = 76.659\n",
      "iteration 27700: training loss = 73.358 validation loss = 76.332\n",
      "iteration 27800: training loss = 75.150 validation loss = 76.794\n",
      "iteration 27900: training loss = 71.775 validation loss = 76.161\n",
      "iteration 28000: training loss = 71.568 validation loss = 76.506\n",
      "iteration 28100: training loss = 73.947 validation loss = 76.741\n",
      "iteration 28200: training loss = 78.540 validation loss = 76.733\n",
      "iteration 28300: training loss = 74.032 validation loss = 76.643\n",
      "iteration 28400: training loss = 72.364 validation loss = 76.580\n",
      "iteration 28500: training loss = 71.403 validation loss = 76.780\n",
      "iteration 28600: training loss = 71.698 validation loss = 76.163\n",
      "iteration 28700: training loss = 69.415 validation loss = 76.630\n",
      "iteration 28800: training loss = 77.247 validation loss = 75.257\n",
      "iteration 28900: training loss = 70.543 validation loss = 76.792\n",
      "iteration 29000: training loss = 73.713 validation loss = 76.423\n",
      "iteration 29100: training loss = 71.931 validation loss = 76.459\n",
      "iteration 29200: training loss = 74.485 validation loss = 76.526\n",
      "iteration 29300: training loss = 76.308 validation loss = 76.852\n",
      "iteration 29400: training loss = 71.132 validation loss = 77.092\n",
      "iteration 29500: training loss = 74.618 validation loss = 75.275\n",
      "iteration 29600: training loss = 72.932 validation loss = 76.954\n",
      "iteration 29700: training loss = 74.317 validation loss = 75.830\n",
      "iteration 29800: training loss = 75.833 validation loss = 75.710\n",
      "iteration 29900: training loss = 75.922 validation loss = 75.927\n",
      "Total weights for  100  neurons:  1020801\n",
      "Time to train:  1251.5677603\n",
      "training loss to be stored:  75.9223403930664\n",
      "validation loss to be stored:  75.92704010009766\n",
      "1\n",
      "Deleting variables, and going to next neuron size\n",
      "======================Linear Model with 2 hidden layers=====================\n",
      "====================== Number of Neurons:  300 ============================\n",
      "nsamples:  15379\n",
      "nbatches:  30\n",
      "nsamples_valid:  3845\n",
      "nbatches_valid:  7\n",
      "iteration 0: training loss = 7681.946 validation loss = 7648.399\n",
      "iteration 100: training loss = 281.229 validation loss = 316.430\n",
      "iteration 200: training loss = 180.037 validation loss = 178.821\n",
      "iteration 300: training loss = 145.354 validation loss = 141.265\n",
      "iteration 400: training loss = 120.392 validation loss = 124.449\n",
      "iteration 500: training loss = 107.392 validation loss = 114.637\n",
      "iteration 600: training loss = 102.990 validation loss = 105.865\n",
      "iteration 700: training loss = 90.850 validation loss = 100.897\n",
      "iteration 800: training loss = 89.467 validation loss = 95.981\n",
      "iteration 900: training loss = 88.256 validation loss = 91.717\n",
      "iteration 1000: training loss = 84.796 validation loss = 88.634\n",
      "iteration 1100: training loss = 79.423 validation loss = 86.605\n",
      "iteration 1200: training loss = 87.340 validation loss = 83.832\n",
      "iteration 1300: training loss = 75.114 validation loss = 82.780\n",
      "iteration 1400: training loss = 79.838 validation loss = 81.432\n",
      "iteration 1500: training loss = 75.470 validation loss = 79.336\n",
      "iteration 1600: training loss = 74.075 validation loss = 77.547\n",
      "iteration 1700: training loss = 70.599 validation loss = 76.439\n",
      "iteration 1800: training loss = 72.978 validation loss = 75.782\n",
      "iteration 1900: training loss = 73.659 validation loss = 74.025\n",
      "iteration 2000: training loss = 68.732 validation loss = 73.589\n",
      "iteration 2100: training loss = 67.904 validation loss = 71.878\n",
      "iteration 2200: training loss = 68.364 validation loss = 71.953\n",
      "iteration 2300: training loss = 70.998 validation loss = 70.320\n",
      "iteration 2400: training loss = 70.042 validation loss = 69.679\n",
      "iteration 2500: training loss = 65.641 validation loss = 69.122\n",
      "iteration 2600: training loss = 62.611 validation loss = 68.358\n",
      "iteration 2700: training loss = 63.931 validation loss = 67.686\n",
      "iteration 2800: training loss = 62.002 validation loss = 66.544\n",
      "iteration 2900: training loss = 70.288 validation loss = 68.706\n",
      "iteration 3000: training loss = 60.976 validation loss = 67.772\n",
      "iteration 3100: training loss = 60.947 validation loss = 65.720\n",
      "iteration 3200: training loss = 60.431 validation loss = 65.061\n",
      "iteration 3300: training loss = 60.733 validation loss = 65.499\n",
      "iteration 3400: training loss = 58.629 validation loss = 64.052\n",
      "iteration 3500: training loss = 62.011 validation loss = 63.463\n",
      "iteration 3600: training loss = 62.618 validation loss = 62.714\n",
      "iteration 3700: training loss = 59.145 validation loss = 66.137\n",
      "iteration 3800: training loss = 62.055 validation loss = 63.908\n",
      "iteration 3900: training loss = 58.657 validation loss = 61.420\n",
      "iteration 4000: training loss = 55.938 validation loss = 61.061\n",
      "iteration 4100: training loss = 59.952 validation loss = 62.679\n",
      "iteration 4200: training loss = 61.846 validation loss = 61.890\n",
      "iteration 4300: training loss = 60.443 validation loss = 59.990\n",
      "iteration 4400: training loss = 57.708 validation loss = 59.597\n",
      "iteration 4500: training loss = 57.679 validation loss = 60.246\n",
      "iteration 4600: training loss = 59.969 validation loss = 59.122\n",
      "iteration 4700: training loss = 57.947 validation loss = 59.544\n",
      "iteration 4800: training loss = 50.921 validation loss = 58.681\n",
      "iteration 4900: training loss = 53.472 validation loss = 61.134\n",
      "iteration 5000: training loss = 54.191 validation loss = 60.308\n",
      "iteration 5100: training loss = 54.796 validation loss = 57.873\n",
      "iteration 5200: training loss = 55.290 validation loss = 58.102\n",
      "iteration 5300: training loss = 56.185 validation loss = 56.803\n",
      "iteration 5400: training loss = 58.229 validation loss = 58.523\n",
      "iteration 5500: training loss = 55.039 validation loss = 56.983\n",
      "iteration 5600: training loss = 52.935 validation loss = 56.929\n",
      "iteration 5700: training loss = 54.392 validation loss = 55.872\n",
      "iteration 5800: training loss = 51.898 validation loss = 56.652\n",
      "iteration 5900: training loss = 55.520 validation loss = 55.703\n",
      "iteration 6000: training loss = 54.922 validation loss = 56.347\n",
      "iteration 6100: training loss = 52.324 validation loss = 56.280\n",
      "iteration 6200: training loss = 53.095 validation loss = 54.296\n",
      "iteration 6300: training loss = 50.179 validation loss = 55.145\n",
      "iteration 6400: training loss = 51.052 validation loss = 55.174\n",
      "iteration 6500: training loss = 52.082 validation loss = 55.865\n",
      "iteration 6600: training loss = 51.987 validation loss = 54.709\n",
      "iteration 6700: training loss = 52.955 validation loss = 56.194\n",
      "iteration 6800: training loss = 56.579 validation loss = 57.656\n",
      "iteration 6900: training loss = 49.245 validation loss = 57.599\n",
      "iteration 7000: training loss = 50.576 validation loss = 57.489\n",
      "iteration 7100: training loss = 50.968 validation loss = 52.944\n",
      "iteration 7200: training loss = 50.511 validation loss = 53.029\n",
      "iteration 7300: training loss = 48.529 validation loss = 53.526\n",
      "iteration 7400: training loss = 51.765 validation loss = 54.995\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 7500: training loss = 47.052 validation loss = 53.249\n",
      "iteration 7600: training loss = 48.813 validation loss = 52.642\n",
      "iteration 7700: training loss = 52.750 validation loss = 53.171\n",
      "iteration 7800: training loss = 51.378 validation loss = 53.150\n",
      "iteration 7900: training loss = 47.513 validation loss = 52.902\n",
      "iteration 8000: training loss = 49.763 validation loss = 51.587\n",
      "iteration 8100: training loss = 49.084 validation loss = 52.516\n",
      "iteration 8200: training loss = 50.214 validation loss = 52.234\n",
      "iteration 8300: training loss = 49.507 validation loss = 52.236\n",
      "iteration 8400: training loss = 50.660 validation loss = 51.183\n",
      "iteration 8500: training loss = 50.002 validation loss = 51.457\n",
      "iteration 8600: training loss = 49.404 validation loss = 53.493\n",
      "iteration 8700: training loss = 47.809 validation loss = 51.199\n",
      "iteration 8800: training loss = 46.755 validation loss = 50.682\n",
      "iteration 8900: training loss = 46.778 validation loss = 50.337\n",
      "iteration 9000: training loss = 48.713 validation loss = 51.408\n",
      "iteration 9100: training loss = 47.865 validation loss = 50.145\n",
      "iteration 9200: training loss = 48.364 validation loss = 51.818\n",
      "iteration 9300: training loss = 46.656 validation loss = 50.082\n",
      "iteration 9400: training loss = 46.564 validation loss = 49.773\n",
      "iteration 9500: training loss = 47.497 validation loss = 50.005\n",
      "iteration 9600: training loss = 48.678 validation loss = 51.026\n",
      "iteration 9700: training loss = 45.127 validation loss = 51.911\n",
      "iteration 9800: training loss = 42.645 validation loss = 49.469\n",
      "iteration 9900: training loss = 46.023 validation loss = 49.474\n",
      "iteration 10000: training loss = 47.111 validation loss = 49.999\n",
      "iteration 10100: training loss = 45.864 validation loss = 49.917\n",
      "iteration 10200: training loss = 49.715 validation loss = 49.658\n",
      "iteration 10300: training loss = 48.482 validation loss = 50.720\n",
      "iteration 10400: training loss = 48.280 validation loss = 51.501\n",
      "iteration 10500: training loss = 44.860 validation loss = 49.557\n",
      "iteration 10600: training loss = 47.309 validation loss = 48.445\n",
      "iteration 10700: training loss = 48.017 validation loss = 48.851\n",
      "iteration 10800: training loss = 44.964 validation loss = 49.017\n",
      "iteration 10900: training loss = 47.451 validation loss = 49.540\n",
      "iteration 11000: training loss = 45.417 validation loss = 47.940\n",
      "iteration 11100: training loss = 46.035 validation loss = 49.142\n",
      "iteration 11200: training loss = 44.131 validation loss = 48.144\n",
      "iteration 11300: training loss = 46.112 validation loss = 47.676\n",
      "iteration 11400: training loss = 45.352 validation loss = 47.860\n",
      "iteration 11500: training loss = 46.116 validation loss = 48.066\n",
      "iteration 11600: training loss = 46.139 validation loss = 48.410\n",
      "iteration 11700: training loss = 43.706 validation loss = 47.207\n",
      "iteration 11800: training loss = 44.989 validation loss = 47.532\n",
      "iteration 11900: training loss = 45.561 validation loss = 47.821\n",
      "iteration 12000: training loss = 44.927 validation loss = 49.087\n",
      "iteration 12100: training loss = 45.648 validation loss = 47.389\n",
      "iteration 12200: training loss = 44.520 validation loss = 47.803\n",
      "iteration 12300: training loss = 43.663 validation loss = 48.454\n",
      "iteration 12400: training loss = 43.642 validation loss = 47.352\n",
      "iteration 12500: training loss = 42.446 validation loss = 46.663\n",
      "iteration 12600: training loss = 44.229 validation loss = 46.908\n",
      "iteration 12700: training loss = 49.051 validation loss = 48.381\n",
      "iteration 12800: training loss = 43.729 validation loss = 46.647\n",
      "iteration 12900: training loss = 44.180 validation loss = 46.638\n",
      "iteration 13000: training loss = 46.283 validation loss = 46.615\n",
      "iteration 13100: training loss = 44.528 validation loss = 46.454\n",
      "iteration 13200: training loss = 43.412 validation loss = 46.187\n",
      "iteration 13300: training loss = 45.068 validation loss = 45.352\n",
      "iteration 13400: training loss = 42.896 validation loss = 46.920\n",
      "iteration 13500: training loss = 42.437 validation loss = 46.242\n",
      "iteration 13600: training loss = 40.250 validation loss = 46.310\n",
      "iteration 13700: training loss = 42.729 validation loss = 45.838\n",
      "iteration 13800: training loss = 44.251 validation loss = 46.308\n",
      "iteration 13900: training loss = 42.369 validation loss = 45.995\n",
      "iteration 14000: training loss = 41.987 validation loss = 45.992\n",
      "iteration 14100: training loss = 44.979 validation loss = 46.287\n",
      "iteration 14200: training loss = 43.059 validation loss = 45.375\n",
      "iteration 14300: training loss = 46.229 validation loss = 45.936\n",
      "iteration 14400: training loss = 42.624 validation loss = 46.305\n",
      "iteration 14500: training loss = 43.366 validation loss = 45.010\n",
      "iteration 14600: training loss = 41.745 validation loss = 45.081\n",
      "iteration 14700: training loss = 42.758 validation loss = 45.033\n",
      "iteration 14800: training loss = 41.195 validation loss = 44.952\n",
      "iteration 14900: training loss = 42.397 validation loss = 44.830\n",
      "iteration 15000: training loss = 42.194 validation loss = 45.229\n",
      "iteration 15100: training loss = 43.208 validation loss = 45.503\n",
      "iteration 15200: training loss = 42.415 validation loss = 45.795\n",
      "iteration 15300: training loss = 41.137 validation loss = 45.082\n",
      "iteration 15400: training loss = 41.109 validation loss = 44.694\n",
      "iteration 15500: training loss = 42.759 validation loss = 44.573\n",
      "iteration 15600: training loss = 41.735 validation loss = 43.785\n",
      "iteration 15700: training loss = 42.724 validation loss = 44.007\n",
      "iteration 15800: training loss = 40.723 validation loss = 45.379\n",
      "iteration 15900: training loss = 41.162 validation loss = 44.688\n",
      "iteration 16000: training loss = 39.213 validation loss = 43.773\n",
      "iteration 16100: training loss = 42.473 validation loss = 44.717\n",
      "iteration 16200: training loss = 40.153 validation loss = 44.075\n",
      "iteration 16300: training loss = 40.252 validation loss = 44.008\n",
      "iteration 16400: training loss = 40.621 validation loss = 43.957\n",
      "iteration 16500: training loss = 40.433 validation loss = 44.310\n",
      "iteration 16600: training loss = 39.105 validation loss = 44.406\n",
      "iteration 16700: training loss = 41.165 validation loss = 43.275\n",
      "iteration 16800: training loss = 37.681 validation loss = 43.237\n",
      "iteration 16900: training loss = 42.204 validation loss = 44.074\n",
      "iteration 17000: training loss = 38.549 validation loss = 43.487\n",
      "iteration 17100: training loss = 39.195 validation loss = 42.900\n",
      "iteration 17200: training loss = 42.408 validation loss = 43.205\n",
      "iteration 17300: training loss = 42.321 validation loss = 43.132\n",
      "iteration 17400: training loss = 40.698 validation loss = 43.293\n",
      "iteration 17500: training loss = 40.983 validation loss = 42.862\n",
      "iteration 17600: training loss = 40.443 validation loss = 44.013\n",
      "iteration 17700: training loss = 38.218 validation loss = 43.173\n",
      "iteration 17800: training loss = 41.302 validation loss = 43.165\n",
      "iteration 17900: training loss = 40.274 validation loss = 42.748\n",
      "iteration 18000: training loss = 40.182 validation loss = 43.624\n",
      "iteration 18100: training loss = 41.631 validation loss = 43.026\n",
      "iteration 18200: training loss = 39.632 validation loss = 43.407\n",
      "iteration 18300: training loss = 39.563 validation loss = 42.516\n",
      "iteration 18400: training loss = 40.201 validation loss = 42.633\n",
      "iteration 18500: training loss = 38.027 validation loss = 42.720\n",
      "iteration 18600: training loss = 38.807 validation loss = 42.592\n",
      "iteration 18700: training loss = 39.843 validation loss = 43.354\n",
      "iteration 18800: training loss = 39.528 validation loss = 42.550\n",
      "iteration 18900: training loss = 40.764 validation loss = 42.236\n",
      "iteration 19000: training loss = 39.365 validation loss = 41.714\n",
      "iteration 19100: training loss = 38.095 validation loss = 42.165\n",
      "iteration 19200: training loss = 41.265 validation loss = 41.620\n",
      "iteration 19300: training loss = 39.285 validation loss = 42.703\n",
      "iteration 19400: training loss = 38.687 validation loss = 42.530\n",
      "iteration 19500: training loss = 38.567 validation loss = 41.618\n",
      "iteration 19600: training loss = 39.622 validation loss = 41.837\n",
      "iteration 19700: training loss = 39.455 validation loss = 42.988\n",
      "iteration 19800: training loss = 36.510 validation loss = 42.004\n",
      "iteration 19900: training loss = 41.082 validation loss = 42.324\n",
      "iteration 20000: training loss = 40.750 validation loss = 41.778\n",
      "iteration 20100: training loss = 37.515 validation loss = 42.050\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 20200: training loss = 39.437 validation loss = 42.335\n",
      "iteration 20300: training loss = 38.312 validation loss = 41.894\n",
      "iteration 20400: training loss = 40.216 validation loss = 41.381\n",
      "iteration 20500: training loss = 40.565 validation loss = 42.108\n",
      "iteration 20600: training loss = 39.335 validation loss = 41.960\n",
      "iteration 20700: training loss = 40.350 validation loss = 42.062\n",
      "iteration 20800: training loss = 38.602 validation loss = 41.497\n",
      "iteration 20900: training loss = 39.496 validation loss = 42.751\n",
      "iteration 21000: training loss = 38.028 validation loss = 41.170\n",
      "iteration 21100: training loss = 40.554 validation loss = 42.314\n",
      "iteration 21200: training loss = 37.605 validation loss = 41.607\n",
      "iteration 21300: training loss = 36.363 validation loss = 40.966\n",
      "iteration 21400: training loss = 36.254 validation loss = 41.216\n",
      "iteration 21500: training loss = 39.242 validation loss = 41.868\n",
      "iteration 21600: training loss = 38.172 validation loss = 40.874\n",
      "iteration 21700: training loss = 41.273 validation loss = 41.196\n",
      "iteration 21800: training loss = 37.114 validation loss = 40.946\n",
      "iteration 21900: training loss = 37.182 validation loss = 40.558\n",
      "iteration 22000: training loss = 38.668 validation loss = 41.132\n",
      "iteration 22100: training loss = 37.613 validation loss = 40.586\n",
      "iteration 22200: training loss = 38.044 validation loss = 40.501\n",
      "iteration 22300: training loss = 37.496 validation loss = 41.452\n",
      "iteration 22400: training loss = 38.116 validation loss = 40.865\n",
      "iteration 22500: training loss = 37.652 validation loss = 40.732\n",
      "iteration 22600: training loss = 37.240 validation loss = 40.540\n",
      "iteration 22700: training loss = 35.612 validation loss = 40.294\n",
      "iteration 22800: training loss = 37.784 validation loss = 40.590\n",
      "iteration 22900: training loss = 36.298 validation loss = 40.249\n",
      "iteration 23000: training loss = 39.880 validation loss = 40.896\n",
      "iteration 23100: training loss = 38.434 validation loss = 40.068\n",
      "iteration 23200: training loss = 38.284 validation loss = 41.202\n",
      "iteration 23300: training loss = 37.831 validation loss = 40.118\n",
      "iteration 23400: training loss = 37.426 validation loss = 40.247\n",
      "iteration 23500: training loss = 36.922 validation loss = 40.539\n",
      "iteration 23600: training loss = 38.154 validation loss = 40.324\n",
      "iteration 23700: training loss = 37.640 validation loss = 39.715\n",
      "iteration 23800: training loss = 35.241 validation loss = 40.915\n",
      "iteration 23900: training loss = 36.500 validation loss = 40.214\n",
      "iteration 24000: training loss = 37.310 validation loss = 40.378\n",
      "iteration 24100: training loss = 37.764 validation loss = 39.561\n",
      "iteration 24200: training loss = 38.051 validation loss = 39.703\n",
      "iteration 24300: training loss = 36.318 validation loss = 39.735\n",
      "iteration 24400: training loss = 34.072 validation loss = 39.717\n",
      "iteration 24500: training loss = 36.711 validation loss = 39.682\n",
      "iteration 24600: training loss = 37.370 validation loss = 40.054\n",
      "iteration 24700: training loss = 37.136 validation loss = 39.630\n",
      "iteration 24800: training loss = 37.561 validation loss = 40.238\n",
      "iteration 24900: training loss = 37.869 validation loss = 40.049\n",
      "iteration 25000: training loss = 37.866 validation loss = 40.595\n",
      "iteration 25100: training loss = 34.430 validation loss = 39.534\n",
      "iteration 25200: training loss = 36.980 validation loss = 39.512\n",
      "iteration 25300: training loss = 36.335 validation loss = 39.024\n",
      "iteration 25400: training loss = 36.133 validation loss = 39.735\n",
      "iteration 25500: training loss = 36.698 validation loss = 40.206\n",
      "iteration 25600: training loss = 36.993 validation loss = 39.756\n",
      "iteration 25700: training loss = 37.107 validation loss = 39.840\n",
      "iteration 25800: training loss = 35.990 validation loss = 39.397\n",
      "iteration 25900: training loss = 36.642 validation loss = 39.180\n",
      "iteration 26000: training loss = 35.498 validation loss = 38.954\n",
      "iteration 26100: training loss = 36.192 validation loss = 39.031\n",
      "iteration 26200: training loss = 36.971 validation loss = 39.083\n",
      "iteration 26300: training loss = 36.682 validation loss = 39.434\n",
      "iteration 26400: training loss = 35.705 validation loss = 40.132\n",
      "iteration 26500: training loss = 36.946 validation loss = 39.231\n",
      "iteration 26600: training loss = 35.944 validation loss = 38.837\n",
      "iteration 26700: training loss = 38.257 validation loss = 39.124\n",
      "iteration 26800: training loss = 33.552 validation loss = 38.442\n",
      "iteration 26900: training loss = 36.847 validation loss = 39.980\n",
      "iteration 27000: training loss = 36.271 validation loss = 38.668\n",
      "iteration 27100: training loss = 34.470 validation loss = 38.126\n",
      "iteration 27200: training loss = 36.727 validation loss = 39.048\n",
      "iteration 27300: training loss = 35.853 validation loss = 39.807\n",
      "iteration 27400: training loss = 35.416 validation loss = 39.191\n",
      "iteration 27500: training loss = 36.695 validation loss = 38.756\n",
      "iteration 27600: training loss = 36.262 validation loss = 38.786\n",
      "iteration 27700: training loss = 36.259 validation loss = 39.351\n",
      "iteration 27800: training loss = 35.674 validation loss = 38.544\n",
      "iteration 27900: training loss = 35.106 validation loss = 39.625\n",
      "iteration 28000: training loss = 33.921 validation loss = 39.553\n",
      "iteration 28100: training loss = 36.398 validation loss = 38.539\n",
      "iteration 28200: training loss = 38.387 validation loss = 38.136\n",
      "iteration 28300: training loss = 33.582 validation loss = 38.229\n",
      "iteration 28400: training loss = 37.204 validation loss = 38.140\n",
      "iteration 28500: training loss = 38.223 validation loss = 38.517\n",
      "iteration 28600: training loss = 35.574 validation loss = 38.108\n",
      "iteration 28700: training loss = 36.091 validation loss = 38.201\n",
      "iteration 28800: training loss = 35.679 validation loss = 37.740\n",
      "iteration 28900: training loss = 36.444 validation loss = 38.633\n",
      "iteration 29000: training loss = 34.517 validation loss = 38.596\n",
      "iteration 29100: training loss = 36.089 validation loss = 39.097\n",
      "iteration 29200: training loss = 35.439 validation loss = 37.705\n",
      "iteration 29300: training loss = 35.735 validation loss = 38.327\n",
      "iteration 29400: training loss = 37.725 validation loss = 38.566\n",
      "iteration 29500: training loss = 33.580 validation loss = 37.936\n",
      "iteration 29600: training loss = 36.055 validation loss = 38.070\n",
      "iteration 29700: training loss = 35.531 validation loss = 37.814\n",
      "iteration 29800: training loss = 35.906 validation loss = 37.521\n",
      "iteration 29900: training loss = 35.089 validation loss = 37.558\n",
      "Total weights for  300  neurons:  3102401\n",
      "Time to train:  1677.1573145\n",
      "training loss to be stored:  35.08924865722656\n",
      "validation loss to be stored:  37.55796432495117\n",
      "2\n",
      "Deleting variables, and going to next neuron size\n",
      "======================Linear Model with 2 hidden layers=====================\n",
      "====================== Number of Neurons:  500 ============================\n",
      "nsamples:  15379\n",
      "nbatches:  30\n",
      "nsamples_valid:  3845\n",
      "nbatches_valid:  7\n",
      "iteration 0: training loss = 7641.507 validation loss = 7627.547\n",
      "iteration 100: training loss = 241.433 validation loss = 241.864\n",
      "iteration 200: training loss = 136.456 validation loss = 147.197\n",
      "iteration 300: training loss = 114.288 validation loss = 118.703\n",
      "iteration 400: training loss = 102.063 validation loss = 103.531\n",
      "iteration 500: training loss = 91.129 validation loss = 94.853\n",
      "iteration 600: training loss = 86.148 validation loss = 88.971\n",
      "iteration 700: training loss = 81.341 validation loss = 85.860\n",
      "iteration 800: training loss = 80.771 validation loss = 83.597\n",
      "iteration 900: training loss = 74.616 validation loss = 77.459\n",
      "iteration 1000: training loss = 72.533 validation loss = 75.864\n",
      "iteration 1100: training loss = 71.437 validation loss = 73.459\n",
      "iteration 1200: training loss = 64.774 validation loss = 72.150\n",
      "iteration 1300: training loss = 70.607 validation loss = 72.946\n",
      "iteration 1400: training loss = 62.617 validation loss = 69.084\n",
      "iteration 1500: training loss = 60.821 validation loss = 67.974\n",
      "iteration 1600: training loss = 62.231 validation loss = 67.338\n",
      "iteration 1700: training loss = 61.456 validation loss = 67.921\n",
      "iteration 1800: training loss = 63.301 validation loss = 64.575\n",
      "iteration 1900: training loss = 59.020 validation loss = 61.852\n",
      "iteration 2000: training loss = 60.585 validation loss = 64.924\n",
      "iteration 2100: training loss = 62.941 validation loss = 61.967\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2200: training loss = 59.424 validation loss = 60.170\n",
      "iteration 2300: training loss = 58.068 validation loss = 59.890\n",
      "iteration 2400: training loss = 58.018 validation loss = 63.278\n",
      "iteration 2500: training loss = 55.637 validation loss = 59.219\n",
      "iteration 2600: training loss = 57.962 validation loss = 57.507\n",
      "iteration 2700: training loss = 54.706 validation loss = 59.176\n",
      "iteration 2800: training loss = 56.740 validation loss = 55.773\n",
      "iteration 2900: training loss = 51.411 validation loss = 55.402\n",
      "iteration 3000: training loss = 53.459 validation loss = 57.042\n",
      "iteration 3100: training loss = 50.258 validation loss = 53.470\n",
      "iteration 3200: training loss = 54.349 validation loss = 54.333\n",
      "iteration 3300: training loss = 53.111 validation loss = 55.440\n",
      "iteration 3400: training loss = 45.405 validation loss = 52.890\n",
      "iteration 3500: training loss = 53.654 validation loss = 54.484\n",
      "iteration 3600: training loss = 51.372 validation loss = 54.159\n",
      "iteration 3700: training loss = 46.710 validation loss = 51.770\n",
      "iteration 3800: training loss = 46.677 validation loss = 51.285\n",
      "iteration 3900: training loss = 45.859 validation loss = 50.312\n",
      "iteration 4000: training loss = 47.339 validation loss = 49.930\n",
      "iteration 4100: training loss = 47.702 validation loss = 51.500\n",
      "iteration 4200: training loss = 48.242 validation loss = 48.382\n",
      "iteration 4300: training loss = 44.767 validation loss = 49.837\n",
      "iteration 4400: training loss = 51.250 validation loss = 50.147\n",
      "iteration 4500: training loss = 47.794 validation loss = 49.014\n",
      "iteration 4600: training loss = 43.668 validation loss = 48.018\n",
      "iteration 4700: training loss = 45.938 validation loss = 48.158\n",
      "iteration 4800: training loss = 43.909 validation loss = 47.191\n",
      "iteration 4900: training loss = 45.696 validation loss = 48.086\n",
      "iteration 5000: training loss = 46.927 validation loss = 46.666\n",
      "iteration 5100: training loss = 44.742 validation loss = 47.384\n",
      "iteration 5200: training loss = 44.940 validation loss = 47.612\n",
      "iteration 5300: training loss = 42.059 validation loss = 45.238\n",
      "iteration 5400: training loss = 42.408 validation loss = 46.705\n",
      "iteration 5500: training loss = 43.474 validation loss = 45.285\n",
      "iteration 5600: training loss = 44.295 validation loss = 44.654\n",
      "iteration 5700: training loss = 40.433 validation loss = 44.970\n",
      "iteration 5800: training loss = 43.770 validation loss = 45.753\n",
      "iteration 5900: training loss = 42.368 validation loss = 44.290\n",
      "iteration 6000: training loss = 40.205 validation loss = 43.122\n",
      "iteration 6100: training loss = 37.922 validation loss = 44.266\n",
      "iteration 6200: training loss = 38.105 validation loss = 43.803\n",
      "iteration 6300: training loss = 38.481 validation loss = 42.929\n",
      "iteration 6400: training loss = 41.786 validation loss = 42.789\n",
      "iteration 6500: training loss = 40.100 validation loss = 42.842\n",
      "iteration 6600: training loss = 41.806 validation loss = 45.345\n",
      "iteration 6700: training loss = 38.124 validation loss = 43.061\n",
      "iteration 6800: training loss = 39.561 validation loss = 41.999\n",
      "iteration 6900: training loss = 38.593 validation loss = 41.049\n",
      "iteration 7000: training loss = 37.850 validation loss = 41.818\n",
      "iteration 7100: training loss = 36.822 validation loss = 40.704\n",
      "iteration 7200: training loss = 38.426 validation loss = 41.562\n",
      "iteration 7300: training loss = 37.473 validation loss = 41.366\n",
      "iteration 7400: training loss = 35.962 validation loss = 41.164\n",
      "iteration 7500: training loss = 40.130 validation loss = 42.069\n",
      "iteration 7600: training loss = 38.282 validation loss = 41.482\n",
      "iteration 7700: training loss = 36.418 validation loss = 40.334\n",
      "iteration 7800: training loss = 37.812 validation loss = 41.206\n",
      "iteration 7900: training loss = 38.454 validation loss = 39.948\n",
      "iteration 8000: training loss = 36.486 validation loss = 40.142\n",
      "iteration 8100: training loss = 40.733 validation loss = 39.892\n",
      "iteration 8200: training loss = 36.972 validation loss = 39.396\n",
      "iteration 8300: training loss = 36.023 validation loss = 38.209\n",
      "iteration 8400: training loss = 35.242 validation loss = 37.946\n",
      "iteration 8500: training loss = 35.147 validation loss = 38.619\n",
      "iteration 8600: training loss = 34.412 validation loss = 38.362\n",
      "iteration 8700: training loss = 35.339 validation loss = 38.440\n",
      "iteration 8800: training loss = 33.534 validation loss = 38.277\n",
      "iteration 8900: training loss = 35.531 validation loss = 37.376\n",
      "iteration 9000: training loss = 35.881 validation loss = 37.332\n",
      "iteration 9100: training loss = 33.485 validation loss = 36.801\n",
      "iteration 9200: training loss = 35.573 validation loss = 37.275\n",
      "iteration 9300: training loss = 33.472 validation loss = 36.647\n",
      "iteration 9400: training loss = 35.362 validation loss = 37.242\n",
      "iteration 9500: training loss = 36.205 validation loss = 37.636\n",
      "iteration 9600: training loss = 32.037 validation loss = 36.673\n",
      "iteration 9700: training loss = 32.509 validation loss = 35.952\n",
      "iteration 9800: training loss = 33.064 validation loss = 37.894\n",
      "iteration 9900: training loss = 35.398 validation loss = 36.996\n",
      "iteration 10000: training loss = 34.139 validation loss = 38.678\n",
      "iteration 10100: training loss = 34.647 validation loss = 37.329\n",
      "iteration 10200: training loss = 32.211 validation loss = 36.440\n",
      "iteration 10300: training loss = 32.382 validation loss = 37.177\n",
      "iteration 10400: training loss = 30.197 validation loss = 35.484\n",
      "iteration 10500: training loss = 33.299 validation loss = 36.038\n",
      "iteration 10600: training loss = 31.401 validation loss = 36.460\n",
      "iteration 10700: training loss = 31.901 validation loss = 35.538\n",
      "iteration 10800: training loss = 31.170 validation loss = 34.791\n",
      "iteration 10900: training loss = 30.826 validation loss = 35.229\n",
      "iteration 11000: training loss = 32.334 validation loss = 35.530\n",
      "iteration 11100: training loss = 31.342 validation loss = 34.819\n",
      "iteration 11200: training loss = 31.678 validation loss = 37.035\n",
      "iteration 11300: training loss = 31.436 validation loss = 34.638\n",
      "iteration 11400: training loss = 31.833 validation loss = 34.486\n",
      "iteration 11500: training loss = 32.669 validation loss = 34.262\n",
      "iteration 11600: training loss = 29.312 validation loss = 34.070\n",
      "iteration 11700: training loss = 32.671 validation loss = 34.271\n",
      "iteration 11800: training loss = 31.531 validation loss = 33.748\n",
      "iteration 11900: training loss = 30.730 validation loss = 35.389\n",
      "iteration 12000: training loss = 31.224 validation loss = 33.506\n",
      "iteration 12100: training loss = 30.077 validation loss = 32.927\n",
      "iteration 12200: training loss = 29.855 validation loss = 33.001\n",
      "iteration 12300: training loss = 30.993 validation loss = 33.539\n",
      "iteration 12400: training loss = 28.001 validation loss = 32.608\n",
      "iteration 12500: training loss = 31.319 validation loss = 34.024\n",
      "iteration 12600: training loss = 30.132 validation loss = 32.586\n",
      "iteration 12700: training loss = 29.466 validation loss = 32.326\n",
      "iteration 12800: training loss = 30.518 validation loss = 32.762\n",
      "iteration 12900: training loss = 30.665 validation loss = 32.978\n",
      "iteration 13000: training loss = 28.354 validation loss = 32.679\n",
      "iteration 13100: training loss = 30.635 validation loss = 32.607\n",
      "iteration 13200: training loss = 31.678 validation loss = 33.616\n",
      "iteration 13300: training loss = 30.807 validation loss = 32.251\n",
      "iteration 13400: training loss = 31.179 validation loss = 32.201\n",
      "iteration 13500: training loss = 29.537 validation loss = 31.657\n",
      "iteration 13600: training loss = 29.431 validation loss = 32.863\n",
      "iteration 13700: training loss = 27.494 validation loss = 31.672\n",
      "iteration 13800: training loss = 28.741 validation loss = 31.997\n",
      "iteration 13900: training loss = 28.901 validation loss = 32.068\n",
      "iteration 14000: training loss = 28.247 validation loss = 31.505\n",
      "iteration 14100: training loss = 27.859 validation loss = 31.793\n",
      "iteration 14200: training loss = 28.671 validation loss = 31.790\n",
      "iteration 14300: training loss = 27.411 validation loss = 31.339\n",
      "iteration 14400: training loss = 31.468 validation loss = 33.017\n",
      "iteration 14500: training loss = 29.103 validation loss = 31.822\n",
      "iteration 14600: training loss = 27.737 validation loss = 31.797\n",
      "iteration 14700: training loss = 27.696 validation loss = 30.897\n",
      "iteration 14800: training loss = 27.340 validation loss = 30.993\n",
      "iteration 14900: training loss = 29.252 validation loss = 31.304\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 15000: training loss = 27.847 validation loss = 31.170\n",
      "iteration 15100: training loss = 29.297 validation loss = 31.829\n",
      "iteration 15200: training loss = 28.141 validation loss = 30.669\n",
      "iteration 15300: training loss = 25.937 validation loss = 30.320\n",
      "iteration 15400: training loss = 28.463 validation loss = 31.606\n",
      "iteration 15500: training loss = 27.774 validation loss = 30.985\n",
      "iteration 15600: training loss = 27.321 validation loss = 31.826\n",
      "iteration 15700: training loss = 26.691 validation loss = 30.554\n",
      "iteration 15800: training loss = 28.028 validation loss = 30.306\n",
      "iteration 15900: training loss = 27.347 validation loss = 30.475\n",
      "iteration 16000: training loss = 28.142 validation loss = 30.677\n",
      "iteration 16100: training loss = 29.002 validation loss = 31.388\n",
      "iteration 16200: training loss = 28.860 validation loss = 31.663\n",
      "iteration 16300: training loss = 27.023 validation loss = 30.710\n",
      "iteration 16400: training loss = 28.179 validation loss = 31.214\n",
      "iteration 16500: training loss = 26.701 validation loss = 29.926\n",
      "iteration 16600: training loss = 25.745 validation loss = 29.726\n",
      "iteration 16700: training loss = 28.236 validation loss = 30.206\n",
      "iteration 16800: training loss = 26.354 validation loss = 30.529\n",
      "iteration 16900: training loss = 27.834 validation loss = 29.939\n",
      "iteration 17000: training loss = 24.529 validation loss = 29.836\n",
      "iteration 17100: training loss = 26.883 validation loss = 30.890\n",
      "iteration 17200: training loss = 26.164 validation loss = 30.594\n",
      "iteration 17300: training loss = 26.324 validation loss = 29.218\n",
      "iteration 17400: training loss = 25.528 validation loss = 29.505\n",
      "iteration 17500: training loss = 27.517 validation loss = 29.896\n",
      "iteration 17600: training loss = 28.036 validation loss = 29.908\n",
      "iteration 17700: training loss = 25.637 validation loss = 29.607\n",
      "iteration 17800: training loss = 26.174 validation loss = 29.757\n",
      "iteration 17900: training loss = 26.296 validation loss = 30.362\n",
      "iteration 18000: training loss = 25.224 validation loss = 29.132\n",
      "iteration 18100: training loss = 26.975 validation loss = 29.304\n",
      "iteration 18200: training loss = 25.959 validation loss = 29.157\n",
      "iteration 18300: training loss = 25.541 validation loss = 29.608\n",
      "iteration 18400: training loss = 25.900 validation loss = 29.107\n",
      "iteration 18500: training loss = 26.648 validation loss = 30.070\n",
      "iteration 18600: training loss = 25.136 validation loss = 28.804\n",
      "iteration 18700: training loss = 26.021 validation loss = 29.449\n",
      "iteration 18800: training loss = 25.485 validation loss = 29.191\n",
      "iteration 18900: training loss = 26.449 validation loss = 28.550\n",
      "iteration 19000: training loss = 24.542 validation loss = 28.599\n",
      "iteration 19100: training loss = 25.105 validation loss = 28.456\n",
      "iteration 19200: training loss = 26.078 validation loss = 29.191\n",
      "iteration 19300: training loss = 25.367 validation loss = 28.891\n",
      "iteration 19400: training loss = 25.660 validation loss = 28.419\n",
      "iteration 19500: training loss = 26.141 validation loss = 28.575\n",
      "iteration 19600: training loss = 25.558 validation loss = 28.476\n",
      "iteration 19700: training loss = 25.505 validation loss = 28.100\n",
      "iteration 19800: training loss = 25.819 validation loss = 28.152\n",
      "iteration 19900: training loss = 24.551 validation loss = 27.978\n",
      "iteration 20000: training loss = 25.577 validation loss = 29.189\n",
      "iteration 20100: training loss = 25.451 validation loss = 28.466\n",
      "iteration 20200: training loss = 25.100 validation loss = 27.987\n",
      "iteration 20300: training loss = 24.130 validation loss = 27.759\n",
      "iteration 20400: training loss = 24.046 validation loss = 27.678\n",
      "iteration 20500: training loss = 26.601 validation loss = 27.903\n",
      "iteration 20600: training loss = 23.944 validation loss = 28.548\n",
      "iteration 20700: training loss = 24.271 validation loss = 28.365\n",
      "iteration 20800: training loss = 24.805 validation loss = 28.480\n",
      "iteration 20900: training loss = 24.846 validation loss = 28.038\n",
      "iteration 21000: training loss = 26.449 validation loss = 28.214\n",
      "iteration 21100: training loss = 26.091 validation loss = 28.280\n",
      "iteration 21200: training loss = 25.011 validation loss = 27.541\n",
      "iteration 21300: training loss = 24.741 validation loss = 27.759\n",
      "iteration 21400: training loss = 25.839 validation loss = 28.392\n",
      "iteration 21500: training loss = 24.177 validation loss = 27.731\n",
      "iteration 21600: training loss = 23.212 validation loss = 27.275\n",
      "iteration 21700: training loss = 25.271 validation loss = 28.224\n",
      "iteration 21800: training loss = 24.459 validation loss = 28.099\n",
      "iteration 21900: training loss = 25.286 validation loss = 28.001\n",
      "iteration 22000: training loss = 25.529 validation loss = 27.781\n",
      "iteration 22100: training loss = 24.309 validation loss = 27.546\n",
      "iteration 22200: training loss = 24.584 validation loss = 27.696\n",
      "iteration 22300: training loss = 23.166 validation loss = 27.282\n",
      "iteration 22400: training loss = 25.116 validation loss = 27.791\n",
      "iteration 22500: training loss = 24.451 validation loss = 27.988\n",
      "iteration 22600: training loss = 25.186 validation loss = 28.708\n",
      "iteration 22700: training loss = 25.774 validation loss = 27.420\n",
      "iteration 22800: training loss = 24.640 validation loss = 27.611\n",
      "iteration 22900: training loss = 24.616 validation loss = 26.825\n",
      "iteration 23000: training loss = 26.346 validation loss = 27.734\n",
      "iteration 23100: training loss = 24.640 validation loss = 27.219\n",
      "iteration 23200: training loss = 24.067 validation loss = 27.709\n",
      "iteration 23300: training loss = 24.564 validation loss = 27.291\n",
      "iteration 23400: training loss = 23.574 validation loss = 27.286\n",
      "iteration 23500: training loss = 23.459 validation loss = 27.275\n",
      "iteration 23600: training loss = 23.641 validation loss = 26.499\n",
      "iteration 23700: training loss = 25.349 validation loss = 28.360\n",
      "iteration 23800: training loss = 23.195 validation loss = 27.073\n",
      "iteration 23900: training loss = 24.144 validation loss = 27.161\n",
      "iteration 24000: training loss = 23.095 validation loss = 27.332\n",
      "iteration 24100: training loss = 24.616 validation loss = 27.011\n",
      "iteration 24200: training loss = 23.355 validation loss = 27.536\n",
      "iteration 24300: training loss = 23.285 validation loss = 26.915\n",
      "iteration 24400: training loss = 23.734 validation loss = 26.491\n",
      "iteration 24500: training loss = 22.947 validation loss = 27.107\n",
      "iteration 24600: training loss = 23.847 validation loss = 26.702\n",
      "iteration 24700: training loss = 23.686 validation loss = 26.621\n",
      "iteration 24800: training loss = 23.641 validation loss = 26.604\n",
      "iteration 24900: training loss = 23.036 validation loss = 26.541\n",
      "iteration 25000: training loss = 22.710 validation loss = 26.336\n",
      "iteration 25100: training loss = 24.452 validation loss = 26.773\n",
      "iteration 25200: training loss = 23.516 validation loss = 26.451\n",
      "iteration 25300: training loss = 23.781 validation loss = 26.771\n",
      "iteration 25400: training loss = 24.268 validation loss = 26.100\n",
      "iteration 25500: training loss = 24.334 validation loss = 26.992\n",
      "iteration 25600: training loss = 23.461 validation loss = 26.570\n",
      "iteration 25700: training loss = 23.380 validation loss = 26.441\n",
      "iteration 25800: training loss = 24.411 validation loss = 26.063\n",
      "iteration 25900: training loss = 23.263 validation loss = 27.606\n",
      "iteration 26000: training loss = 22.176 validation loss = 26.437\n",
      "iteration 26100: training loss = 24.210 validation loss = 26.919\n",
      "iteration 26200: training loss = 22.610 validation loss = 26.987\n",
      "iteration 26300: training loss = 23.452 validation loss = 26.429\n",
      "iteration 26400: training loss = 23.493 validation loss = 26.467\n",
      "iteration 26500: training loss = 22.598 validation loss = 26.188\n",
      "iteration 26600: training loss = 23.892 validation loss = 26.293\n",
      "iteration 26700: training loss = 23.025 validation loss = 26.714\n",
      "iteration 26800: training loss = 23.505 validation loss = 27.178\n",
      "iteration 26900: training loss = 22.508 validation loss = 26.704\n",
      "iteration 27000: training loss = 21.922 validation loss = 26.123\n",
      "iteration 27100: training loss = 22.921 validation loss = 25.880\n",
      "iteration 27200: training loss = 22.617 validation loss = 25.637\n",
      "iteration 27300: training loss = 22.515 validation loss = 25.724\n",
      "iteration 27400: training loss = 24.765 validation loss = 26.362\n",
      "iteration 27500: training loss = 23.699 validation loss = 25.922\n",
      "iteration 27600: training loss = 22.391 validation loss = 25.841\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 27700: training loss = 22.205 validation loss = 26.101\n",
      "iteration 27800: training loss = 22.351 validation loss = 25.770\n",
      "iteration 27900: training loss = 22.899 validation loss = 25.836\n",
      "iteration 28000: training loss = 23.211 validation loss = 25.365\n",
      "iteration 28100: training loss = 22.981 validation loss = 26.612\n",
      "iteration 28200: training loss = 23.602 validation loss = 25.414\n",
      "iteration 28300: training loss = 22.644 validation loss = 25.949\n",
      "iteration 28400: training loss = 23.844 validation loss = 25.972\n",
      "iteration 28500: training loss = 22.514 validation loss = 25.507\n",
      "iteration 28600: training loss = 23.008 validation loss = 25.957\n",
      "iteration 28700: training loss = 23.398 validation loss = 25.941\n",
      "iteration 28800: training loss = 22.474 validation loss = 25.345\n",
      "iteration 28900: training loss = 24.147 validation loss = 25.947\n",
      "iteration 29000: training loss = 23.322 validation loss = 25.447\n",
      "iteration 29100: training loss = 22.730 validation loss = 25.866\n",
      "iteration 29200: training loss = 20.808 validation loss = 25.237\n",
      "iteration 29300: training loss = 21.422 validation loss = 25.796\n",
      "iteration 29400: training loss = 21.155 validation loss = 25.299\n",
      "iteration 29500: training loss = 23.000 validation loss = 25.557\n",
      "iteration 29600: training loss = 22.224 validation loss = 25.140\n",
      "iteration 29700: training loss = 23.765 validation loss = 25.909\n",
      "iteration 29800: training loss = 23.014 validation loss = 25.169\n",
      "iteration 29900: training loss = 23.630 validation loss = 25.611\n",
      "Total weights for  500  neurons:  5264001\n",
      "Time to train:  2080.6932402\n",
      "training loss to be stored:  23.630300521850586\n",
      "validation loss to be stored:  25.611234664916992\n",
      "3\n",
      "Deleting variables, and going to next neuron size\n",
      "======================Linear Model with 2 hidden layers=====================\n",
      "====================== Number of Neurons:  700 ============================\n",
      "nsamples:  15379\n",
      "nbatches:  30\n",
      "nsamples_valid:  3845\n",
      "nbatches_valid:  7\n",
      "iteration 0: training loss = 7613.094 validation loss = 7555.958\n",
      "iteration 100: training loss = 196.882 validation loss = 213.548\n",
      "iteration 200: training loss = 126.351 validation loss = 134.250\n",
      "iteration 300: training loss = 103.158 validation loss = 110.558\n",
      "iteration 400: training loss = 88.409 validation loss = 96.824\n",
      "iteration 500: training loss = 87.454 validation loss = 90.645\n",
      "iteration 600: training loss = 83.188 validation loss = 82.756\n",
      "iteration 700: training loss = 73.822 validation loss = 77.906\n",
      "iteration 800: training loss = 69.322 validation loss = 76.654\n",
      "iteration 900: training loss = 71.124 validation loss = 76.236\n",
      "iteration 1000: training loss = 70.528 validation loss = 71.405\n",
      "iteration 1100: training loss = 66.618 validation loss = 67.782\n",
      "iteration 1200: training loss = 64.760 validation loss = 68.133\n",
      "iteration 1300: training loss = 62.339 validation loss = 64.881\n",
      "iteration 1400: training loss = 60.646 validation loss = 65.094\n",
      "iteration 1500: training loss = 57.373 validation loss = 61.749\n",
      "iteration 1600: training loss = 67.031 validation loss = 66.687\n",
      "iteration 1700: training loss = 63.412 validation loss = 62.113\n",
      "iteration 1800: training loss = 64.033 validation loss = 61.924\n",
      "iteration 1900: training loss = 61.547 validation loss = 63.467\n",
      "iteration 2000: training loss = 62.672 validation loss = 63.017\n",
      "iteration 2100: training loss = 61.276 validation loss = 60.323\n",
      "iteration 2200: training loss = 52.260 validation loss = 58.096\n",
      "iteration 2300: training loss = 51.054 validation loss = 55.680\n",
      "iteration 2400: training loss = 54.164 validation loss = 56.044\n",
      "iteration 2500: training loss = 52.073 validation loss = 53.766\n",
      "iteration 2600: training loss = 49.470 validation loss = 54.059\n",
      "iteration 2700: training loss = 49.129 validation loss = 53.876\n",
      "iteration 2800: training loss = 46.647 validation loss = 51.952\n",
      "iteration 2900: training loss = 51.363 validation loss = 52.034\n",
      "iteration 3000: training loss = 49.642 validation loss = 50.077\n",
      "iteration 3100: training loss = 49.970 validation loss = 49.816\n",
      "iteration 3200: training loss = 46.241 validation loss = 51.511\n",
      "iteration 3300: training loss = 44.265 validation loss = 48.635\n",
      "iteration 3400: training loss = 44.272 validation loss = 46.528\n",
      "iteration 3500: training loss = 47.464 validation loss = 49.246\n",
      "iteration 3600: training loss = 41.638 validation loss = 45.823\n",
      "iteration 3700: training loss = 47.713 validation loss = 45.843\n",
      "iteration 3800: training loss = 44.424 validation loss = 44.430\n",
      "iteration 3900: training loss = 40.063 validation loss = 44.254\n",
      "iteration 4000: training loss = 42.120 validation loss = 44.353\n",
      "iteration 4100: training loss = 43.967 validation loss = 44.768\n",
      "iteration 4200: training loss = 37.395 validation loss = 42.471\n",
      "iteration 4300: training loss = 42.227 validation loss = 42.604\n",
      "iteration 4400: training loss = 37.116 validation loss = 41.278\n",
      "iteration 4500: training loss = 38.687 validation loss = 41.536\n",
      "iteration 4600: training loss = 40.008 validation loss = 40.369\n",
      "iteration 4700: training loss = 38.749 validation loss = 43.590\n",
      "iteration 4800: training loss = 36.710 validation loss = 40.118\n",
      "iteration 4900: training loss = 36.778 validation loss = 39.973\n",
      "iteration 5000: training loss = 39.097 validation loss = 40.248\n",
      "iteration 5100: training loss = 36.464 validation loss = 40.320\n",
      "iteration 5200: training loss = 34.632 validation loss = 38.807\n",
      "iteration 5300: training loss = 33.326 validation loss = 38.588\n",
      "iteration 5400: training loss = 34.683 validation loss = 38.061\n",
      "iteration 5500: training loss = 36.678 validation loss = 40.802\n",
      "iteration 5600: training loss = 35.391 validation loss = 38.018\n",
      "iteration 5700: training loss = 36.693 validation loss = 37.075\n",
      "iteration 5800: training loss = 32.375 validation loss = 36.547\n",
      "iteration 5900: training loss = 34.268 validation loss = 36.139\n",
      "iteration 6000: training loss = 33.413 validation loss = 38.300\n",
      "iteration 6100: training loss = 32.156 validation loss = 36.141\n",
      "iteration 6200: training loss = 34.968 validation loss = 36.577\n",
      "iteration 6300: training loss = 32.971 validation loss = 35.673\n",
      "iteration 6400: training loss = 33.949 validation loss = 35.756\n",
      "iteration 6500: training loss = 32.024 validation loss = 35.733\n",
      "iteration 6600: training loss = 31.656 validation loss = 35.165\n",
      "iteration 6700: training loss = 30.750 validation loss = 33.862\n",
      "iteration 6800: training loss = 31.603 validation loss = 35.403\n",
      "iteration 6900: training loss = 29.828 validation loss = 34.868\n",
      "iteration 7000: training loss = 32.409 validation loss = 34.904\n",
      "iteration 7100: training loss = 30.709 validation loss = 35.735\n",
      "iteration 7200: training loss = 29.714 validation loss = 35.202\n",
      "iteration 7300: training loss = 29.104 validation loss = 34.462\n",
      "iteration 7400: training loss = 28.312 validation loss = 32.238\n",
      "iteration 7500: training loss = 30.799 validation loss = 33.007\n",
      "iteration 7600: training loss = 29.752 validation loss = 32.632\n",
      "iteration 7700: training loss = 28.325 validation loss = 32.166\n",
      "iteration 7800: training loss = 32.301 validation loss = 34.275\n",
      "iteration 7900: training loss = 29.601 validation loss = 33.157\n",
      "iteration 8000: training loss = 27.519 validation loss = 32.337\n",
      "iteration 8100: training loss = 26.576 validation loss = 32.408\n",
      "iteration 8200: training loss = 27.558 validation loss = 32.028\n",
      "iteration 8300: training loss = 27.708 validation loss = 31.669\n",
      "iteration 8400: training loss = 29.688 validation loss = 30.837\n",
      "iteration 8500: training loss = 28.730 validation loss = 31.863\n",
      "iteration 8600: training loss = 28.069 validation loss = 30.831\n",
      "iteration 8700: training loss = 30.900 validation loss = 31.985\n",
      "iteration 8800: training loss = 27.357 validation loss = 30.422\n",
      "iteration 8900: training loss = 29.772 validation loss = 31.560\n",
      "iteration 9000: training loss = 26.294 validation loss = 30.627\n",
      "iteration 9100: training loss = 27.082 validation loss = 29.912\n",
      "iteration 9200: training loss = 27.156 validation loss = 30.786\n",
      "iteration 9300: training loss = 27.346 validation loss = 30.218\n",
      "iteration 9400: training loss = 28.334 validation loss = 31.559\n",
      "iteration 9500: training loss = 27.189 validation loss = 29.826\n",
      "iteration 9600: training loss = 27.962 validation loss = 29.538\n",
      "iteration 9700: training loss = 28.457 validation loss = 31.366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 9800: training loss = 27.444 validation loss = 31.818\n",
      "iteration 9900: training loss = 26.853 validation loss = 31.215\n",
      "iteration 10000: training loss = 25.089 validation loss = 29.280\n",
      "iteration 10100: training loss = 25.781 validation loss = 29.881\n",
      "iteration 10200: training loss = 25.932 validation loss = 30.241\n",
      "iteration 10300: training loss = 24.594 validation loss = 28.897\n",
      "iteration 10400: training loss = 26.020 validation loss = 30.214\n",
      "iteration 10500: training loss = 25.830 validation loss = 29.226\n",
      "iteration 10600: training loss = 25.638 validation loss = 28.866\n",
      "iteration 10700: training loss = 25.152 validation loss = 28.732\n",
      "iteration 10800: training loss = 26.474 validation loss = 29.600\n",
      "iteration 10900: training loss = 27.538 validation loss = 30.404\n",
      "iteration 11000: training loss = 24.422 validation loss = 28.502\n",
      "iteration 11100: training loss = 25.579 validation loss = 27.713\n",
      "iteration 11200: training loss = 24.764 validation loss = 28.787\n",
      "iteration 11300: training loss = 25.794 validation loss = 29.309\n",
      "iteration 11400: training loss = 24.867 validation loss = 28.050\n",
      "iteration 11500: training loss = 25.204 validation loss = 28.137\n",
      "iteration 11600: training loss = 23.148 validation loss = 28.569\n",
      "iteration 11700: training loss = 26.998 validation loss = 29.052\n",
      "iteration 11800: training loss = 25.435 validation loss = 29.931\n",
      "iteration 11900: training loss = 24.133 validation loss = 26.884\n",
      "iteration 12000: training loss = 26.125 validation loss = 28.735\n",
      "iteration 12100: training loss = 24.326 validation loss = 29.086\n",
      "iteration 12200: training loss = 24.844 validation loss = 28.569\n",
      "iteration 12300: training loss = 23.004 validation loss = 27.086\n",
      "iteration 12400: training loss = 23.812 validation loss = 27.164\n",
      "iteration 12500: training loss = 23.114 validation loss = 27.839\n",
      "iteration 12600: training loss = 22.610 validation loss = 27.173\n",
      "iteration 12700: training loss = 24.877 validation loss = 27.579\n",
      "iteration 12800: training loss = 25.893 validation loss = 28.000\n",
      "iteration 12900: training loss = 23.761 validation loss = 27.605\n",
      "iteration 13000: training loss = 23.478 validation loss = 27.808\n",
      "iteration 13100: training loss = 22.427 validation loss = 26.339\n",
      "iteration 13200: training loss = 22.712 validation loss = 26.912\n",
      "iteration 13300: training loss = 22.155 validation loss = 27.112\n",
      "iteration 13400: training loss = 21.972 validation loss = 26.476\n",
      "iteration 13500: training loss = 23.846 validation loss = 27.559\n",
      "iteration 13600: training loss = 22.185 validation loss = 26.941\n",
      "iteration 13700: training loss = 22.490 validation loss = 25.958\n",
      "iteration 13800: training loss = 21.833 validation loss = 26.535\n",
      "iteration 13900: training loss = 21.324 validation loss = 25.427\n",
      "iteration 14000: training loss = 23.412 validation loss = 26.474\n",
      "iteration 14100: training loss = 22.188 validation loss = 26.004\n",
      "iteration 14200: training loss = 23.805 validation loss = 27.306\n",
      "iteration 14300: training loss = 22.885 validation loss = 25.786\n",
      "iteration 14400: training loss = 21.917 validation loss = 26.060\n",
      "iteration 14500: training loss = 21.479 validation loss = 26.592\n",
      "iteration 14600: training loss = 22.791 validation loss = 26.269\n",
      "iteration 14700: training loss = 21.840 validation loss = 24.974\n",
      "iteration 14800: training loss = 22.318 validation loss = 25.958\n",
      "iteration 14900: training loss = 22.939 validation loss = 27.177\n",
      "iteration 15000: training loss = 20.880 validation loss = 25.531\n",
      "iteration 15100: training loss = 20.370 validation loss = 24.835\n",
      "iteration 15200: training loss = 22.291 validation loss = 26.209\n",
      "iteration 15300: training loss = 20.167 validation loss = 25.169\n",
      "iteration 15400: training loss = 21.846 validation loss = 24.948\n",
      "iteration 15500: training loss = 21.751 validation loss = 25.428\n",
      "iteration 15600: training loss = 21.346 validation loss = 26.079\n",
      "iteration 15700: training loss = 20.450 validation loss = 25.289\n",
      "iteration 15800: training loss = 21.770 validation loss = 25.523\n",
      "iteration 15900: training loss = 21.470 validation loss = 25.568\n",
      "iteration 16000: training loss = 23.390 validation loss = 25.497\n",
      "iteration 16100: training loss = 21.927 validation loss = 26.386\n",
      "iteration 16200: training loss = 21.061 validation loss = 24.787\n",
      "iteration 16300: training loss = 20.033 validation loss = 25.302\n",
      "iteration 16400: training loss = 23.312 validation loss = 25.735\n",
      "iteration 16500: training loss = 21.069 validation loss = 26.074\n",
      "iteration 16600: training loss = 20.190 validation loss = 25.732\n",
      "iteration 16700: training loss = 20.815 validation loss = 24.624\n",
      "iteration 16800: training loss = 20.905 validation loss = 25.159\n",
      "iteration 16900: training loss = 20.643 validation loss = 25.193\n",
      "iteration 17000: training loss = 20.687 validation loss = 24.342\n",
      "iteration 17100: training loss = 21.265 validation loss = 25.329\n",
      "iteration 17200: training loss = 20.293 validation loss = 24.159\n",
      "iteration 17300: training loss = 21.987 validation loss = 24.695\n",
      "iteration 17400: training loss = 22.005 validation loss = 25.205\n",
      "iteration 17500: training loss = 22.232 validation loss = 24.920\n",
      "iteration 17600: training loss = 19.779 validation loss = 25.048\n",
      "iteration 17700: training loss = 19.541 validation loss = 24.255\n",
      "iteration 17800: training loss = 21.651 validation loss = 25.207\n",
      "iteration 17900: training loss = 20.171 validation loss = 23.937\n",
      "iteration 18000: training loss = 21.467 validation loss = 24.590\n",
      "iteration 18100: training loss = 20.400 validation loss = 24.112\n",
      "iteration 18200: training loss = 21.073 validation loss = 25.373\n",
      "iteration 18300: training loss = 20.960 validation loss = 24.290\n",
      "iteration 18400: training loss = 20.288 validation loss = 24.080\n",
      "iteration 18500: training loss = 19.786 validation loss = 24.601\n",
      "iteration 18600: training loss = 20.011 validation loss = 23.176\n",
      "iteration 18700: training loss = 20.225 validation loss = 23.661\n",
      "iteration 18800: training loss = 21.641 validation loss = 24.098\n",
      "iteration 18900: training loss = 20.845 validation loss = 24.279\n",
      "iteration 19000: training loss = 21.763 validation loss = 23.946\n",
      "iteration 19100: training loss = 21.422 validation loss = 24.725\n",
      "iteration 19200: training loss = 20.564 validation loss = 24.698\n",
      "iteration 19300: training loss = 20.075 validation loss = 24.149\n",
      "iteration 19400: training loss = 19.760 validation loss = 24.048\n",
      "iteration 19500: training loss = 19.481 validation loss = 23.966\n",
      "iteration 19600: training loss = 21.228 validation loss = 24.046\n",
      "iteration 19700: training loss = 19.376 validation loss = 24.042\n",
      "iteration 19800: training loss = 20.116 validation loss = 23.940\n",
      "iteration 19900: training loss = 20.378 validation loss = 24.347\n",
      "iteration 20000: training loss = 20.721 validation loss = 23.836\n",
      "iteration 20100: training loss = 19.207 validation loss = 22.773\n",
      "iteration 20200: training loss = 19.650 validation loss = 23.405\n",
      "iteration 20300: training loss = 19.425 validation loss = 23.445\n",
      "iteration 20400: training loss = 19.564 validation loss = 23.135\n",
      "iteration 20500: training loss = 19.797 validation loss = 22.840\n",
      "iteration 20600: training loss = 19.245 validation loss = 23.363\n",
      "iteration 20700: training loss = 18.422 validation loss = 23.510\n",
      "iteration 20800: training loss = 19.143 validation loss = 22.990\n",
      "iteration 20900: training loss = 20.719 validation loss = 23.291\n",
      "iteration 21000: training loss = 19.841 validation loss = 22.996\n",
      "iteration 21100: training loss = 19.651 validation loss = 24.320\n",
      "iteration 21200: training loss = 19.671 validation loss = 23.541\n",
      "iteration 21300: training loss = 19.294 validation loss = 22.813\n",
      "iteration 21400: training loss = 18.455 validation loss = 23.389\n",
      "iteration 21500: training loss = 19.118 validation loss = 22.759\n",
      "iteration 21600: training loss = 19.111 validation loss = 22.765\n",
      "iteration 21700: training loss = 19.281 validation loss = 22.907\n",
      "iteration 21800: training loss = 18.696 validation loss = 23.051\n",
      "iteration 21900: training loss = 19.633 validation loss = 22.990\n",
      "iteration 22000: training loss = 19.752 validation loss = 22.850\n",
      "iteration 22100: training loss = 19.152 validation loss = 22.231\n",
      "iteration 22200: training loss = 18.457 validation loss = 22.798\n",
      "iteration 22300: training loss = 18.767 validation loss = 22.289\n",
      "iteration 22400: training loss = 19.299 validation loss = 23.400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 22500: training loss = 19.453 validation loss = 22.598\n",
      "iteration 22600: training loss = 17.883 validation loss = 22.427\n",
      "iteration 22700: training loss = 20.084 validation loss = 22.846\n",
      "iteration 22800: training loss = 19.004 validation loss = 23.432\n",
      "iteration 22900: training loss = 18.390 validation loss = 22.111\n",
      "iteration 23000: training loss = 20.135 validation loss = 24.396\n",
      "iteration 23100: training loss = 19.911 validation loss = 23.520\n",
      "iteration 23200: training loss = 19.451 validation loss = 22.713\n",
      "iteration 23300: training loss = 19.236 validation loss = 22.707\n",
      "iteration 23400: training loss = 17.977 validation loss = 22.575\n",
      "iteration 23500: training loss = 18.124 validation loss = 22.217\n",
      "iteration 23600: training loss = 18.556 validation loss = 22.811\n",
      "iteration 23700: training loss = 18.271 validation loss = 22.985\n",
      "iteration 23800: training loss = 18.116 validation loss = 22.181\n",
      "iteration 23900: training loss = 18.371 validation loss = 22.392\n",
      "iteration 24000: training loss = 18.521 validation loss = 22.556\n",
      "iteration 24100: training loss = 19.760 validation loss = 23.211\n",
      "iteration 24200: training loss = 19.206 validation loss = 22.222\n",
      "iteration 24300: training loss = 21.379 validation loss = 23.965\n",
      "iteration 24400: training loss = 17.872 validation loss = 23.030\n",
      "iteration 24500: training loss = 19.170 validation loss = 22.938\n",
      "iteration 24600: training loss = 19.081 validation loss = 22.315\n",
      "iteration 24700: training loss = 18.367 validation loss = 22.199\n",
      "iteration 24800: training loss = 18.449 validation loss = 22.234\n",
      "iteration 24900: training loss = 18.377 validation loss = 21.607\n",
      "iteration 25000: training loss = 19.569 validation loss = 23.308\n",
      "iteration 25100: training loss = 18.704 validation loss = 22.193\n",
      "iteration 25200: training loss = 19.878 validation loss = 23.126\n",
      "iteration 25300: training loss = 19.158 validation loss = 22.345\n",
      "iteration 25400: training loss = 17.980 validation loss = 22.019\n",
      "iteration 25500: training loss = 18.648 validation loss = 22.692\n",
      "iteration 25600: training loss = 18.333 validation loss = 22.653\n",
      "iteration 25700: training loss = 18.532 validation loss = 22.066\n",
      "iteration 25800: training loss = 19.844 validation loss = 21.844\n",
      "iteration 25900: training loss = 18.835 validation loss = 22.137\n",
      "iteration 26000: training loss = 16.972 validation loss = 21.630\n",
      "iteration 26100: training loss = 18.538 validation loss = 21.823\n",
      "iteration 26200: training loss = 17.752 validation loss = 24.520\n",
      "iteration 26300: training loss = 18.440 validation loss = 23.275\n",
      "iteration 26400: training loss = 18.351 validation loss = 22.516\n",
      "iteration 26500: training loss = 18.271 validation loss = 21.285\n",
      "iteration 26600: training loss = 17.943 validation loss = 21.695\n",
      "iteration 26700: training loss = 18.422 validation loss = 22.229\n",
      "iteration 26800: training loss = 19.052 validation loss = 22.401\n",
      "iteration 26900: training loss = 18.113 validation loss = 22.007\n",
      "iteration 27000: training loss = 16.807 validation loss = 21.306\n",
      "iteration 27100: training loss = 18.917 validation loss = 21.675\n",
      "iteration 27200: training loss = 18.184 validation loss = 22.148\n",
      "iteration 27300: training loss = 16.964 validation loss = 21.621\n",
      "iteration 27400: training loss = 18.095 validation loss = 21.972\n",
      "iteration 27500: training loss = 17.016 validation loss = 21.635\n",
      "iteration 27600: training loss = 16.908 validation loss = 21.829\n",
      "iteration 27700: training loss = 16.622 validation loss = 21.863\n",
      "iteration 27800: training loss = 17.868 validation loss = 21.685\n",
      "iteration 27900: training loss = 17.578 validation loss = 21.917\n",
      "iteration 28000: training loss = 16.985 validation loss = 21.526\n",
      "iteration 28100: training loss = 17.552 validation loss = 21.801\n",
      "iteration 28200: training loss = 17.523 validation loss = 21.835\n",
      "iteration 28300: training loss = 17.124 validation loss = 21.290\n",
      "iteration 28400: training loss = 17.281 validation loss = 21.850\n",
      "iteration 28500: training loss = 17.995 validation loss = 21.441\n",
      "iteration 28600: training loss = 17.600 validation loss = 21.428\n",
      "iteration 28700: training loss = 17.262 validation loss = 21.318\n",
      "iteration 28800: training loss = 17.620 validation loss = 20.865\n",
      "iteration 28900: training loss = 19.125 validation loss = 21.683\n",
      "iteration 29000: training loss = 17.500 validation loss = 20.850\n",
      "iteration 29100: training loss = 16.116 validation loss = 20.822\n",
      "iteration 29200: training loss = 16.851 validation loss = 21.656\n",
      "iteration 29300: training loss = 17.850 validation loss = 21.043\n",
      "iteration 29400: training loss = 16.677 validation loss = 21.360\n",
      "iteration 29500: training loss = 16.665 validation loss = 20.856\n",
      "iteration 29600: training loss = 17.359 validation loss = 21.786\n",
      "iteration 29700: training loss = 16.097 validation loss = 20.768\n",
      "iteration 29800: training loss = 17.146 validation loss = 20.971\n",
      "iteration 29900: training loss = 18.031 validation loss = 21.325\n",
      "Total weights for  700  neurons:  7505601\n",
      "Time to train:  2936.5737701\n",
      "training loss to be stored:  18.03068733215332\n",
      "validation loss to be stored:  21.324554443359375\n",
      "4\n",
      "Deleting variables, and going to next neuron size\n",
      "======================Linear Model with 2 hidden layers=====================\n",
      "====================== Number of Neurons:  900 ============================\n",
      "nsamples:  15379\n",
      "nbatches:  30\n",
      "nsamples_valid:  3845\n",
      "nbatches_valid:  7\n",
      "iteration 0: training loss = 7810.709 validation loss = 7542.253\n",
      "iteration 100: training loss = 178.162 validation loss = 184.334\n",
      "iteration 200: training loss = 120.134 validation loss = 122.747\n",
      "iteration 300: training loss = 98.061 validation loss = 103.306\n",
      "iteration 400: training loss = 83.608 validation loss = 88.640\n",
      "iteration 500: training loss = 81.051 validation loss = 84.535\n",
      "iteration 600: training loss = 74.791 validation loss = 76.367\n",
      "iteration 700: training loss = 67.595 validation loss = 72.302\n",
      "iteration 800: training loss = 78.035 validation loss = 75.101\n",
      "iteration 900: training loss = 72.594 validation loss = 72.623\n",
      "iteration 1000: training loss = 70.834 validation loss = 70.489\n",
      "iteration 1100: training loss = 73.041 validation loss = 69.675\n",
      "iteration 1200: training loss = 65.835 validation loss = 65.775\n",
      "iteration 1300: training loss = 63.879 validation loss = 62.694\n",
      "iteration 1400: training loss = 62.392 validation loss = 62.561\n",
      "iteration 1500: training loss = 63.622 validation loss = 62.585\n",
      "iteration 1600: training loss = 56.371 validation loss = 54.703\n",
      "iteration 1700: training loss = 52.809 validation loss = 56.818\n",
      "iteration 1800: training loss = 49.683 validation loss = 55.461\n",
      "iteration 1900: training loss = 53.171 validation loss = 55.571\n",
      "iteration 2000: training loss = 46.957 validation loss = 49.842\n",
      "iteration 2100: training loss = 51.849 validation loss = 54.152\n",
      "iteration 2200: training loss = 51.506 validation loss = 50.734\n",
      "iteration 2300: training loss = 47.080 validation loss = 50.566\n",
      "iteration 2400: training loss = 44.485 validation loss = 48.777\n",
      "iteration 2500: training loss = 44.232 validation loss = 45.736\n",
      "iteration 2600: training loss = 48.714 validation loss = 46.102\n",
      "iteration 2700: training loss = 43.390 validation loss = 44.978\n",
      "iteration 2800: training loss = 38.518 validation loss = 43.671\n",
      "iteration 2900: training loss = 39.937 validation loss = 46.414\n",
      "iteration 3000: training loss = 39.276 validation loss = 42.125\n",
      "iteration 3100: training loss = 37.434 validation loss = 40.740\n",
      "iteration 3200: training loss = 39.717 validation loss = 41.887\n",
      "iteration 3300: training loss = 35.594 validation loss = 40.170\n",
      "iteration 3400: training loss = 37.862 validation loss = 40.360\n",
      "iteration 3500: training loss = 35.268 validation loss = 39.603\n",
      "iteration 3600: training loss = 34.967 validation loss = 39.568\n",
      "iteration 3700: training loss = 33.680 validation loss = 38.132\n",
      "iteration 3800: training loss = 31.873 validation loss = 36.933\n",
      "iteration 3900: training loss = 38.779 validation loss = 37.971\n",
      "iteration 4000: training loss = 31.139 validation loss = 35.405\n",
      "iteration 4100: training loss = 32.449 validation loss = 36.913\n",
      "iteration 4200: training loss = 30.722 validation loss = 35.922\n",
      "iteration 4300: training loss = 30.789 validation loss = 34.575\n",
      "iteration 4400: training loss = 31.425 validation loss = 34.867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 4500: training loss = 31.411 validation loss = 36.669\n",
      "iteration 4600: training loss = 28.841 validation loss = 34.529\n",
      "iteration 4700: training loss = 31.543 validation loss = 36.121\n",
      "iteration 4800: training loss = 30.139 validation loss = 33.905\n",
      "iteration 4900: training loss = 27.920 validation loss = 31.687\n",
      "iteration 5000: training loss = 29.907 validation loss = 33.751\n",
      "iteration 5100: training loss = 29.095 validation loss = 33.859\n",
      "iteration 5200: training loss = 29.072 validation loss = 32.528\n",
      "iteration 5300: training loss = 26.858 validation loss = 32.377\n",
      "iteration 5400: training loss = 26.726 validation loss = 32.725\n",
      "iteration 5500: training loss = 27.862 validation loss = 30.322\n",
      "iteration 5600: training loss = 26.380 validation loss = 31.750\n",
      "iteration 5700: training loss = 26.705 validation loss = 30.935\n",
      "iteration 5800: training loss = 26.147 validation loss = 30.064\n",
      "iteration 5900: training loss = 25.421 validation loss = 29.306\n",
      "iteration 6000: training loss = 28.578 validation loss = 31.167\n",
      "iteration 6100: training loss = 25.943 validation loss = 29.901\n",
      "iteration 6200: training loss = 26.348 validation loss = 30.651\n",
      "iteration 6300: training loss = 26.721 validation loss = 29.644\n",
      "iteration 6400: training loss = 24.786 validation loss = 29.227\n",
      "iteration 6500: training loss = 25.865 validation loss = 31.410\n",
      "iteration 6600: training loss = 23.991 validation loss = 28.154\n",
      "iteration 6700: training loss = 25.998 validation loss = 30.219\n",
      "iteration 6800: training loss = 23.737 validation loss = 28.662\n",
      "iteration 6900: training loss = 23.764 validation loss = 27.569\n",
      "iteration 7000: training loss = 25.192 validation loss = 28.765\n",
      "iteration 7100: training loss = 23.815 validation loss = 29.315\n",
      "iteration 7200: training loss = 23.413 validation loss = 26.767\n",
      "iteration 7300: training loss = 22.739 validation loss = 28.157\n",
      "iteration 7400: training loss = 25.717 validation loss = 28.018\n",
      "iteration 7500: training loss = 23.861 validation loss = 28.670\n",
      "iteration 7600: training loss = 23.702 validation loss = 26.880\n",
      "iteration 7700: training loss = 22.965 validation loss = 27.461\n",
      "iteration 7800: training loss = 23.775 validation loss = 26.898\n",
      "iteration 7900: training loss = 23.191 validation loss = 26.342\n",
      "iteration 8000: training loss = 23.302 validation loss = 27.553\n",
      "iteration 8100: training loss = 21.722 validation loss = 26.030\n",
      "iteration 8200: training loss = 22.416 validation loss = 26.106\n",
      "iteration 8300: training loss = 23.352 validation loss = 25.535\n",
      "iteration 8400: training loss = 21.903 validation loss = 26.779\n",
      "iteration 8500: training loss = 23.281 validation loss = 26.882\n",
      "iteration 8600: training loss = 22.022 validation loss = 25.955\n",
      "iteration 8700: training loss = 20.549 validation loss = 25.968\n",
      "iteration 8800: training loss = 21.926 validation loss = 26.298\n",
      "iteration 8900: training loss = 19.950 validation loss = 26.161\n",
      "iteration 9000: training loss = 22.234 validation loss = 25.938\n",
      "iteration 9100: training loss = 21.483 validation loss = 24.987\n",
      "iteration 9200: training loss = 22.457 validation loss = 26.051\n",
      "iteration 9300: training loss = 21.936 validation loss = 26.882\n",
      "iteration 9400: training loss = 21.798 validation loss = 27.596\n",
      "iteration 9500: training loss = 23.588 validation loss = 26.620\n",
      "iteration 9600: training loss = 21.461 validation loss = 26.344\n",
      "iteration 9700: training loss = 19.409 validation loss = 24.594\n",
      "iteration 9800: training loss = 20.012 validation loss = 23.825\n",
      "iteration 9900: training loss = 21.394 validation loss = 25.723\n",
      "iteration 10000: training loss = 19.807 validation loss = 24.163\n",
      "iteration 10100: training loss = 21.399 validation loss = 25.346\n",
      "iteration 10200: training loss = 20.637 validation loss = 24.030\n",
      "iteration 10300: training loss = 19.786 validation loss = 24.180\n",
      "iteration 10400: training loss = 20.938 validation loss = 24.648\n",
      "iteration 10500: training loss = 21.001 validation loss = 24.517\n",
      "iteration 10600: training loss = 19.988 validation loss = 23.816\n",
      "iteration 10700: training loss = 23.287 validation loss = 25.153\n",
      "iteration 10800: training loss = 20.381 validation loss = 24.640\n",
      "iteration 10900: training loss = 20.771 validation loss = 24.236\n",
      "iteration 11000: training loss = 20.630 validation loss = 26.013\n",
      "iteration 11100: training loss = 20.311 validation loss = 23.623\n",
      "iteration 11200: training loss = 19.363 validation loss = 23.167\n",
      "iteration 11300: training loss = 19.305 validation loss = 23.768\n",
      "iteration 11400: training loss = 19.710 validation loss = 23.004\n",
      "iteration 11500: training loss = 22.155 validation loss = 24.980\n",
      "iteration 11600: training loss = 19.594 validation loss = 24.030\n",
      "iteration 11700: training loss = 18.968 validation loss = 25.037\n",
      "iteration 11800: training loss = 19.323 validation loss = 24.127\n",
      "iteration 11900: training loss = 17.135 validation loss = 22.157\n",
      "iteration 12000: training loss = 19.590 validation loss = 22.878\n",
      "iteration 12100: training loss = 19.921 validation loss = 23.982\n",
      "iteration 12200: training loss = 20.765 validation loss = 23.927\n",
      "iteration 12300: training loss = 18.842 validation loss = 22.264\n",
      "iteration 12400: training loss = 19.693 validation loss = 23.613\n",
      "iteration 12500: training loss = 19.346 validation loss = 22.574\n",
      "iteration 12600: training loss = 19.714 validation loss = 23.886\n",
      "iteration 12700: training loss = 19.883 validation loss = 22.338\n",
      "iteration 12800: training loss = 18.924 validation loss = 22.715\n",
      "iteration 12900: training loss = 20.722 validation loss = 23.093\n",
      "iteration 13000: training loss = 18.073 validation loss = 22.783\n",
      "iteration 13100: training loss = 18.368 validation loss = 23.339\n",
      "iteration 13200: training loss = 19.334 validation loss = 24.849\n",
      "iteration 13300: training loss = 17.112 validation loss = 22.665\n",
      "iteration 13400: training loss = 19.957 validation loss = 22.857\n",
      "iteration 13500: training loss = 17.923 validation loss = 21.598\n",
      "iteration 13600: training loss = 22.407 validation loss = 23.129\n",
      "iteration 13700: training loss = 20.461 validation loss = 23.743\n",
      "iteration 13800: training loss = 17.175 validation loss = 22.063\n",
      "iteration 13900: training loss = 17.642 validation loss = 22.638\n",
      "iteration 14000: training loss = 18.005 validation loss = 22.302\n",
      "iteration 14100: training loss = 17.544 validation loss = 21.891\n",
      "iteration 14200: training loss = 18.731 validation loss = 23.132\n",
      "iteration 14300: training loss = 17.997 validation loss = 22.180\n",
      "iteration 14400: training loss = 17.222 validation loss = 23.295\n",
      "iteration 14500: training loss = 18.485 validation loss = 22.406\n",
      "iteration 14600: training loss = 18.930 validation loss = 22.723\n",
      "iteration 14700: training loss = 17.274 validation loss = 22.007\n",
      "iteration 14800: training loss = 20.418 validation loss = 22.499\n",
      "iteration 14900: training loss = 16.949 validation loss = 20.974\n",
      "iteration 15000: training loss = 17.072 validation loss = 21.139\n",
      "iteration 15100: training loss = 20.039 validation loss = 22.985\n",
      "iteration 15200: training loss = 16.761 validation loss = 21.877\n",
      "iteration 15300: training loss = 18.083 validation loss = 22.284\n",
      "iteration 15400: training loss = 16.904 validation loss = 22.389\n",
      "iteration 15500: training loss = 20.649 validation loss = 23.550\n",
      "iteration 15600: training loss = 18.275 validation loss = 22.116\n",
      "iteration 15700: training loss = 18.307 validation loss = 21.557\n",
      "iteration 15800: training loss = 17.281 validation loss = 21.718\n",
      "iteration 15900: training loss = 17.937 validation loss = 21.559\n",
      "iteration 16000: training loss = 18.215 validation loss = 22.459\n",
      "iteration 16100: training loss = 16.259 validation loss = 21.656\n",
      "iteration 16200: training loss = 18.434 validation loss = 21.927\n",
      "iteration 16300: training loss = 18.951 validation loss = 21.129\n",
      "iteration 16400: training loss = 17.595 validation loss = 22.231\n",
      "iteration 16500: training loss = 16.389 validation loss = 21.085\n",
      "iteration 16600: training loss = 17.134 validation loss = 20.475\n",
      "iteration 16700: training loss = 19.286 validation loss = 22.514\n",
      "iteration 16800: training loss = 17.536 validation loss = 21.727\n",
      "iteration 16900: training loss = 17.839 validation loss = 20.787\n",
      "iteration 17000: training loss = 18.052 validation loss = 22.274\n",
      "iteration 17100: training loss = 16.983 validation loss = 20.925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 17200: training loss = 17.360 validation loss = 20.820\n",
      "iteration 17300: training loss = 16.173 validation loss = 20.459\n",
      "iteration 17400: training loss = 16.574 validation loss = 21.908\n",
      "iteration 17500: training loss = 17.474 validation loss = 22.026\n",
      "iteration 17600: training loss = 18.714 validation loss = 24.455\n",
      "iteration 17700: training loss = 17.669 validation loss = 21.181\n",
      "iteration 17800: training loss = 16.922 validation loss = 22.111\n",
      "iteration 17900: training loss = 17.352 validation loss = 22.299\n",
      "iteration 18000: training loss = 17.341 validation loss = 21.199\n",
      "iteration 18100: training loss = 16.221 validation loss = 21.431\n",
      "iteration 18200: training loss = 17.070 validation loss = 20.658\n",
      "iteration 18300: training loss = 16.598 validation loss = 20.275\n",
      "iteration 18400: training loss = 16.801 validation loss = 20.957\n",
      "iteration 18500: training loss = 16.354 validation loss = 20.365\n",
      "iteration 18600: training loss = 17.306 validation loss = 21.722\n",
      "iteration 18700: training loss = 16.103 validation loss = 20.711\n",
      "iteration 18800: training loss = 15.925 validation loss = 20.350\n",
      "iteration 18900: training loss = 16.336 validation loss = 21.108\n",
      "iteration 19000: training loss = 18.075 validation loss = 21.537\n",
      "iteration 19100: training loss = 16.488 validation loss = 20.222\n",
      "iteration 19200: training loss = 16.682 validation loss = 21.225\n",
      "iteration 19300: training loss = 17.767 validation loss = 20.578\n",
      "iteration 19400: training loss = 17.053 validation loss = 20.642\n",
      "iteration 19500: training loss = 16.069 validation loss = 21.546\n",
      "iteration 19600: training loss = 16.013 validation loss = 20.511\n",
      "iteration 19700: training loss = 16.850 validation loss = 21.312\n",
      "iteration 19800: training loss = 17.359 validation loss = 20.670\n",
      "iteration 19900: training loss = 17.006 validation loss = 19.807\n",
      "iteration 20000: training loss = 16.881 validation loss = 20.610\n",
      "iteration 20100: training loss = 15.961 validation loss = 20.434\n",
      "iteration 20200: training loss = 15.902 validation loss = 20.372\n",
      "iteration 20300: training loss = 16.628 validation loss = 20.273\n",
      "iteration 20400: training loss = 16.178 validation loss = 20.112\n",
      "iteration 20500: training loss = 15.183 validation loss = 19.700\n",
      "iteration 20600: training loss = 15.590 validation loss = 19.959\n",
      "iteration 20700: training loss = 15.105 validation loss = 19.397\n",
      "iteration 20800: training loss = 15.415 validation loss = 20.296\n",
      "iteration 20900: training loss = 14.652 validation loss = 20.257\n",
      "iteration 21000: training loss = 17.650 validation loss = 20.404\n",
      "iteration 21100: training loss = 15.611 validation loss = 19.459\n",
      "iteration 21200: training loss = 15.817 validation loss = 21.042\n",
      "iteration 21300: training loss = 15.149 validation loss = 19.793\n",
      "iteration 21400: training loss = 15.963 validation loss = 19.823\n",
      "iteration 21500: training loss = 15.561 validation loss = 20.401\n",
      "iteration 21600: training loss = 15.951 validation loss = 20.083\n",
      "iteration 21700: training loss = 15.450 validation loss = 19.389\n",
      "iteration 21800: training loss = 15.052 validation loss = 19.271\n",
      "iteration 21900: training loss = 15.974 validation loss = 19.371\n",
      "iteration 22000: training loss = 16.250 validation loss = 19.664\n",
      "iteration 22100: training loss = 15.572 validation loss = 20.660\n",
      "iteration 22200: training loss = 15.602 validation loss = 19.654\n",
      "iteration 22300: training loss = 14.755 validation loss = 19.025\n",
      "iteration 22400: training loss = 16.131 validation loss = 19.807\n",
      "iteration 22500: training loss = 16.455 validation loss = 20.159\n",
      "iteration 22600: training loss = 15.926 validation loss = 19.740\n",
      "iteration 22700: training loss = 16.727 validation loss = 20.701\n",
      "iteration 22800: training loss = 15.039 validation loss = 19.731\n",
      "iteration 22900: training loss = 16.071 validation loss = 19.461\n",
      "iteration 23000: training loss = 16.554 validation loss = 20.342\n",
      "iteration 23100: training loss = 14.489 validation loss = 19.026\n",
      "iteration 23200: training loss = 16.820 validation loss = 20.336\n",
      "iteration 23300: training loss = 14.660 validation loss = 19.369\n",
      "iteration 23400: training loss = 16.181 validation loss = 19.132\n",
      "iteration 23500: training loss = 15.754 validation loss = 21.858\n",
      "iteration 23600: training loss = 15.786 validation loss = 20.370\n",
      "iteration 23700: training loss = 14.928 validation loss = 19.285\n",
      "iteration 23800: training loss = 16.365 validation loss = 20.017\n",
      "iteration 23900: training loss = 16.069 validation loss = 19.474\n",
      "iteration 24000: training loss = 15.646 validation loss = 19.575\n",
      "iteration 24100: training loss = 14.580 validation loss = 18.838\n",
      "iteration 24200: training loss = 15.249 validation loss = 20.476\n",
      "iteration 24300: training loss = 13.678 validation loss = 18.661\n",
      "iteration 24400: training loss = 14.258 validation loss = 18.811\n",
      "iteration 24500: training loss = 15.059 validation loss = 19.363\n",
      "iteration 24600: training loss = 15.630 validation loss = 19.757\n",
      "iteration 24700: training loss = 15.117 validation loss = 19.334\n",
      "iteration 24800: training loss = 14.627 validation loss = 19.361\n",
      "iteration 24900: training loss = 14.079 validation loss = 18.492\n",
      "iteration 25000: training loss = 14.332 validation loss = 18.958\n",
      "iteration 25100: training loss = 14.587 validation loss = 19.451\n",
      "iteration 25200: training loss = 14.970 validation loss = 19.635\n",
      "iteration 25300: training loss = 15.349 validation loss = 18.922\n",
      "iteration 25400: training loss = 14.656 validation loss = 19.289\n",
      "iteration 25500: training loss = 14.611 validation loss = 18.862\n",
      "iteration 25600: training loss = 13.730 validation loss = 19.345\n",
      "iteration 25700: training loss = 15.289 validation loss = 18.813\n",
      "iteration 25800: training loss = 13.469 validation loss = 19.020\n",
      "iteration 25900: training loss = 13.817 validation loss = 19.015\n",
      "iteration 26000: training loss = 14.791 validation loss = 18.627\n",
      "iteration 26100: training loss = 14.597 validation loss = 18.717\n",
      "iteration 26200: training loss = 14.765 validation loss = 19.536\n",
      "iteration 26300: training loss = 14.420 validation loss = 18.496\n",
      "iteration 26400: training loss = 14.159 validation loss = 19.212\n",
      "iteration 26500: training loss = 14.063 validation loss = 18.594\n",
      "iteration 26600: training loss = 14.833 validation loss = 19.507\n",
      "iteration 26700: training loss = 14.603 validation loss = 19.028\n",
      "iteration 26800: training loss = 13.854 validation loss = 18.296\n",
      "iteration 26900: training loss = 17.079 validation loss = 20.126\n",
      "iteration 27000: training loss = 14.166 validation loss = 18.314\n",
      "iteration 27100: training loss = 15.559 validation loss = 19.394\n",
      "iteration 27200: training loss = 16.309 validation loss = 19.388\n",
      "iteration 27300: training loss = 13.838 validation loss = 18.254\n",
      "iteration 27400: training loss = 15.573 validation loss = 19.606\n",
      "iteration 27500: training loss = 14.437 validation loss = 18.800\n",
      "iteration 27600: training loss = 15.181 validation loss = 18.900\n",
      "iteration 27700: training loss = 13.341 validation loss = 18.595\n",
      "iteration 27800: training loss = 14.266 validation loss = 19.086\n",
      "iteration 27900: training loss = 14.779 validation loss = 18.219\n",
      "iteration 28000: training loss = 15.575 validation loss = 18.939\n",
      "iteration 28100: training loss = 14.624 validation loss = 18.997\n",
      "iteration 28200: training loss = 14.223 validation loss = 18.640\n",
      "iteration 28300: training loss = 15.163 validation loss = 19.028\n",
      "iteration 28400: training loss = 13.915 validation loss = 18.898\n",
      "iteration 28500: training loss = 13.853 validation loss = 18.968\n",
      "iteration 28600: training loss = 15.492 validation loss = 19.026\n",
      "iteration 28700: training loss = 15.067 validation loss = 18.817\n",
      "iteration 28800: training loss = 15.146 validation loss = 19.267\n",
      "iteration 28900: training loss = 15.995 validation loss = 20.425\n",
      "iteration 29000: training loss = 14.132 validation loss = 18.576\n",
      "iteration 29100: training loss = 15.233 validation loss = 18.960\n",
      "iteration 29200: training loss = 14.296 validation loss = 19.017\n",
      "iteration 29300: training loss = 14.788 validation loss = 18.344\n",
      "iteration 29400: training loss = 15.318 validation loss = 19.964\n",
      "iteration 29500: training loss = 13.345 validation loss = 17.921\n",
      "iteration 29600: training loss = 14.278 validation loss = 18.413\n",
      "iteration 29700: training loss = 14.643 validation loss = 18.721\n",
      "iteration 29800: training loss = 14.266 validation loss = 18.798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 29900: training loss = 14.827 validation loss = 19.799\n",
      "Total weights for  900  neurons:  9827201\n",
      "Time to train:  3633.2867183999997\n",
      "training loss to be stored:  14.827181816101074\n",
      "validation loss to be stored:  19.79935646057129\n",
      "5\n",
      "Deleting variables, and going to next neuron size\n",
      "======================Linear Model with 2 hidden layers=====================\n",
      "====================== Number of Neurons:  1100 ============================\n",
      "nsamples:  15379\n",
      "nbatches:  30\n",
      "nsamples_valid:  3845\n",
      "nbatches_valid:  7\n",
      "iteration 0: training loss = 7624.293 validation loss = 7499.890\n",
      "iteration 100: training loss = 174.326 validation loss = 172.156\n",
      "iteration 200: training loss = 111.413 validation loss = 115.102\n",
      "iteration 300: training loss = 97.781 validation loss = 93.256\n",
      "iteration 400: training loss = 80.731 validation loss = 86.504\n",
      "iteration 500: training loss = 79.841 validation loss = 80.391\n",
      "iteration 600: training loss = 80.171 validation loss = 76.273\n",
      "iteration 700: training loss = 76.150 validation loss = 74.759\n",
      "iteration 800: training loss = 76.793 validation loss = 74.614\n",
      "iteration 900: training loss = 72.129 validation loss = 71.351\n",
      "iteration 1000: training loss = 67.935 validation loss = 66.311\n",
      "iteration 1100: training loss = 59.823 validation loss = 60.188\n",
      "iteration 1200: training loss = 59.172 validation loss = 62.207\n",
      "iteration 1300: training loss = 57.169 validation loss = 59.420\n",
      "iteration 1400: training loss = 50.600 validation loss = 55.606\n",
      "iteration 1500: training loss = 49.820 validation loss = 57.497\n",
      "iteration 1600: training loss = 50.135 validation loss = 53.769\n",
      "iteration 1700: training loss = 49.164 validation loss = 53.297\n",
      "iteration 1800: training loss = 56.486 validation loss = 55.323\n",
      "iteration 1900: training loss = 38.182 validation loss = 43.577\n",
      "iteration 2000: training loss = 45.612 validation loss = 44.426\n",
      "iteration 2100: training loss = 46.045 validation loss = 46.730\n",
      "iteration 2200: training loss = 39.914 validation loss = 44.098\n",
      "iteration 2300: training loss = 38.310 validation loss = 44.719\n",
      "iteration 2400: training loss = 36.855 validation loss = 40.654\n",
      "iteration 2500: training loss = 37.615 validation loss = 41.863\n",
      "iteration 2600: training loss = 36.984 validation loss = 39.950\n",
      "iteration 2700: training loss = 36.610 validation loss = 38.428\n",
      "iteration 2800: training loss = 33.243 validation loss = 37.944\n",
      "iteration 2900: training loss = 34.990 validation loss = 37.818\n",
      "iteration 3000: training loss = 31.940 validation loss = 35.656\n",
      "iteration 3100: training loss = 33.125 validation loss = 37.193\n",
      "iteration 3200: training loss = 30.018 validation loss = 34.204\n",
      "iteration 3300: training loss = 31.989 validation loss = 35.824\n",
      "iteration 3400: training loss = 33.091 validation loss = 35.217\n",
      "iteration 3500: training loss = 29.189 validation loss = 32.515\n",
      "iteration 3600: training loss = 29.229 validation loss = 33.908\n",
      "iteration 3700: training loss = 28.700 validation loss = 35.236\n",
      "iteration 3800: training loss = 30.842 validation loss = 34.531\n",
      "iteration 3900: training loss = 26.603 validation loss = 33.475\n",
      "iteration 4000: training loss = 27.956 validation loss = 32.625\n",
      "iteration 4100: training loss = 28.213 validation loss = 34.314\n",
      "iteration 4200: training loss = 26.707 validation loss = 30.362\n",
      "iteration 4300: training loss = 25.724 validation loss = 30.270\n",
      "iteration 4400: training loss = 26.113 validation loss = 29.918\n",
      "iteration 4500: training loss = 26.468 validation loss = 29.631\n",
      "iteration 4600: training loss = 30.437 validation loss = 31.711\n",
      "iteration 4700: training loss = 26.418 validation loss = 29.748\n",
      "iteration 4800: training loss = 26.135 validation loss = 29.965\n",
      "iteration 4900: training loss = 23.860 validation loss = 28.426\n",
      "iteration 5000: training loss = 24.176 validation loss = 28.141\n",
      "iteration 5100: training loss = 24.840 validation loss = 29.632\n",
      "iteration 5200: training loss = 24.044 validation loss = 27.680\n",
      "iteration 5300: training loss = 22.970 validation loss = 26.823\n",
      "iteration 5400: training loss = 24.568 validation loss = 27.196\n",
      "iteration 5500: training loss = 21.718 validation loss = 29.456\n",
      "iteration 5600: training loss = 23.154 validation loss = 26.570\n",
      "iteration 5700: training loss = 23.859 validation loss = 28.891\n",
      "iteration 5800: training loss = 25.589 validation loss = 29.970\n",
      "iteration 5900: training loss = 24.025 validation loss = 28.191\n",
      "iteration 6000: training loss = 23.764 validation loss = 30.611\n",
      "iteration 6100: training loss = 21.359 validation loss = 25.352\n",
      "iteration 6200: training loss = 22.953 validation loss = 26.882\n",
      "iteration 6300: training loss = 22.066 validation loss = 26.180\n",
      "iteration 6400: training loss = 21.519 validation loss = 26.308\n",
      "iteration 6500: training loss = 22.716 validation loss = 26.494\n",
      "iteration 6600: training loss = 19.755 validation loss = 24.137\n",
      "iteration 6700: training loss = 23.962 validation loss = 24.761\n",
      "iteration 6800: training loss = 23.019 validation loss = 27.083\n",
      "iteration 6900: training loss = 22.852 validation loss = 25.855\n",
      "iteration 7000: training loss = 21.198 validation loss = 25.698\n",
      "iteration 7100: training loss = 25.340 validation loss = 27.247\n",
      "iteration 7200: training loss = 22.780 validation loss = 27.854\n",
      "iteration 7300: training loss = 22.174 validation loss = 25.466\n",
      "iteration 7400: training loss = 22.220 validation loss = 25.608\n",
      "iteration 7500: training loss = 19.144 validation loss = 24.644\n",
      "iteration 7600: training loss = 19.821 validation loss = 23.698\n",
      "iteration 7700: training loss = 21.099 validation loss = 25.366\n",
      "iteration 7800: training loss = 20.747 validation loss = 24.024\n",
      "iteration 7900: training loss = 19.410 validation loss = 25.561\n",
      "iteration 8000: training loss = 19.563 validation loss = 23.530\n",
      "iteration 8100: training loss = 20.897 validation loss = 24.510\n",
      "iteration 8200: training loss = 19.492 validation loss = 23.743\n",
      "iteration 8300: training loss = 20.673 validation loss = 26.093\n",
      "iteration 8400: training loss = 19.507 validation loss = 23.266\n",
      "iteration 8500: training loss = 18.048 validation loss = 23.912\n",
      "iteration 8600: training loss = 20.452 validation loss = 23.541\n",
      "iteration 8700: training loss = 18.802 validation loss = 22.653\n",
      "iteration 8800: training loss = 20.755 validation loss = 23.414\n",
      "iteration 8900: training loss = 17.959 validation loss = 22.572\n",
      "iteration 9000: training loss = 19.442 validation loss = 23.532\n",
      "iteration 9100: training loss = 19.291 validation loss = 24.274\n",
      "iteration 9200: training loss = 17.975 validation loss = 22.305\n",
      "iteration 9300: training loss = 20.208 validation loss = 24.061\n",
      "iteration 9400: training loss = 18.439 validation loss = 22.917\n",
      "iteration 9500: training loss = 20.182 validation loss = 23.887\n",
      "iteration 9600: training loss = 16.645 validation loss = 22.618\n",
      "iteration 9700: training loss = 21.859 validation loss = 24.744\n",
      "iteration 9800: training loss = 17.688 validation loss = 23.093\n",
      "iteration 9900: training loss = 17.213 validation loss = 21.242\n",
      "iteration 10000: training loss = 19.536 validation loss = 23.049\n",
      "iteration 10100: training loss = 19.232 validation loss = 21.783\n",
      "iteration 10200: training loss = 16.130 validation loss = 21.177\n",
      "iteration 10300: training loss = 18.741 validation loss = 22.808\n",
      "iteration 10400: training loss = 17.252 validation loss = 21.120\n",
      "iteration 10500: training loss = 17.025 validation loss = 21.428\n",
      "iteration 10600: training loss = 16.574 validation loss = 21.341\n",
      "iteration 10700: training loss = 20.706 validation loss = 25.819\n",
      "iteration 10800: training loss = 17.436 validation loss = 22.334\n",
      "iteration 10900: training loss = 17.656 validation loss = 21.932\n",
      "iteration 11000: training loss = 15.674 validation loss = 20.831\n",
      "iteration 11100: training loss = 17.380 validation loss = 24.337\n",
      "iteration 11200: training loss = 17.870 validation loss = 21.856\n",
      "iteration 11300: training loss = 18.160 validation loss = 21.381\n",
      "iteration 11400: training loss = 21.310 validation loss = 25.810\n",
      "iteration 11500: training loss = 16.671 validation loss = 21.363\n",
      "iteration 11600: training loss = 17.766 validation loss = 21.404\n",
      "iteration 11700: training loss = 17.014 validation loss = 22.760\n",
      "iteration 11800: training loss = 16.304 validation loss = 20.223\n",
      "iteration 11900: training loss = 16.193 validation loss = 20.282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 12000: training loss = 15.503 validation loss = 21.433\n",
      "iteration 12100: training loss = 16.710 validation loss = 21.635\n",
      "iteration 12200: training loss = 16.358 validation loss = 23.230\n",
      "iteration 12300: training loss = 16.104 validation loss = 21.179\n",
      "iteration 12400: training loss = 15.802 validation loss = 20.857\n",
      "iteration 12500: training loss = 16.322 validation loss = 21.147\n",
      "iteration 12600: training loss = 16.332 validation loss = 20.715\n",
      "iteration 12700: training loss = 16.224 validation loss = 20.605\n",
      "iteration 12800: training loss = 16.196 validation loss = 20.533\n",
      "iteration 12900: training loss = 15.501 validation loss = 21.123\n",
      "iteration 13000: training loss = 18.476 validation loss = 21.476\n",
      "iteration 13100: training loss = 15.078 validation loss = 20.394\n",
      "iteration 13200: training loss = 18.010 validation loss = 20.765\n",
      "iteration 13300: training loss = 15.363 validation loss = 20.228\n",
      "iteration 13400: training loss = 18.760 validation loss = 20.683\n",
      "iteration 13500: training loss = 15.746 validation loss = 20.545\n",
      "iteration 13600: training loss = 15.658 validation loss = 22.893\n",
      "iteration 13700: training loss = 15.650 validation loss = 19.964\n",
      "iteration 13800: training loss = 16.038 validation loss = 20.174\n",
      "iteration 13900: training loss = 16.675 validation loss = 20.043\n",
      "iteration 14000: training loss = 16.125 validation loss = 21.641\n",
      "iteration 14100: training loss = 15.132 validation loss = 20.140\n",
      "iteration 14200: training loss = 15.654 validation loss = 22.115\n",
      "iteration 14300: training loss = 16.976 validation loss = 20.935\n",
      "iteration 14400: training loss = 16.751 validation loss = 21.668\n",
      "iteration 14500: training loss = 15.915 validation loss = 20.272\n",
      "iteration 14600: training loss = 15.045 validation loss = 22.193\n",
      "iteration 14700: training loss = 15.707 validation loss = 19.351\n",
      "iteration 14800: training loss = 15.839 validation loss = 19.540\n",
      "iteration 14900: training loss = 14.036 validation loss = 20.469\n",
      "iteration 15000: training loss = 14.906 validation loss = 20.059\n",
      "iteration 15100: training loss = 14.116 validation loss = 19.490\n",
      "iteration 15200: training loss = 15.382 validation loss = 20.779\n",
      "iteration 15300: training loss = 16.562 validation loss = 20.890\n",
      "iteration 15400: training loss = 14.730 validation loss = 19.023\n",
      "iteration 15500: training loss = 14.116 validation loss = 18.781\n",
      "iteration 15600: training loss = 16.015 validation loss = 19.863\n",
      "iteration 15700: training loss = 15.771 validation loss = 20.397\n",
      "iteration 15800: training loss = 14.813 validation loss = 19.060\n",
      "iteration 15900: training loss = 14.328 validation loss = 18.735\n",
      "iteration 16000: training loss = 17.124 validation loss = 18.956\n",
      "iteration 16100: training loss = 16.171 validation loss = 20.062\n",
      "iteration 16200: training loss = 14.666 validation loss = 19.501\n",
      "iteration 16300: training loss = 15.402 validation loss = 20.983\n",
      "iteration 16400: training loss = 14.666 validation loss = 19.787\n",
      "iteration 16500: training loss = 17.605 validation loss = 20.538\n",
      "iteration 16600: training loss = 14.676 validation loss = 19.128\n",
      "iteration 16700: training loss = 13.785 validation loss = 18.552\n",
      "iteration 16800: training loss = 14.695 validation loss = 19.209\n",
      "iteration 16900: training loss = 16.127 validation loss = 19.044\n",
      "iteration 17000: training loss = 15.268 validation loss = 18.491\n",
      "iteration 17100: training loss = 14.263 validation loss = 18.152\n",
      "iteration 17200: training loss = 14.631 validation loss = 19.502\n",
      "iteration 17300: training loss = 15.707 validation loss = 20.351\n",
      "iteration 17400: training loss = 16.707 validation loss = 20.717\n",
      "iteration 17500: training loss = 15.541 validation loss = 19.182\n",
      "iteration 17600: training loss = 14.539 validation loss = 18.857\n",
      "iteration 17700: training loss = 13.792 validation loss = 18.767\n",
      "iteration 17800: training loss = 14.741 validation loss = 19.184\n",
      "iteration 17900: training loss = 13.297 validation loss = 19.406\n",
      "iteration 18000: training loss = 15.590 validation loss = 19.167\n",
      "iteration 18100: training loss = 15.098 validation loss = 19.296\n",
      "iteration 18200: training loss = 14.356 validation loss = 18.598\n",
      "iteration 18300: training loss = 13.977 validation loss = 20.404\n",
      "iteration 18400: training loss = 14.732 validation loss = 18.891\n",
      "iteration 18500: training loss = 15.708 validation loss = 20.868\n",
      "iteration 18600: training loss = 14.492 validation loss = 20.034\n",
      "iteration 18700: training loss = 14.293 validation loss = 19.120\n",
      "iteration 18800: training loss = 14.590 validation loss = 19.968\n",
      "iteration 18900: training loss = 13.698 validation loss = 18.875\n",
      "iteration 19000: training loss = 16.332 validation loss = 19.281\n",
      "iteration 19100: training loss = 13.988 validation loss = 19.241\n",
      "iteration 19200: training loss = 14.310 validation loss = 19.616\n",
      "iteration 19300: training loss = 14.677 validation loss = 19.913\n",
      "iteration 19400: training loss = 13.417 validation loss = 17.719\n",
      "iteration 19500: training loss = 15.030 validation loss = 18.580\n",
      "iteration 19600: training loss = 13.164 validation loss = 18.403\n",
      "iteration 19700: training loss = 13.330 validation loss = 17.904\n",
      "iteration 19800: training loss = 14.608 validation loss = 19.544\n",
      "iteration 19900: training loss = 13.710 validation loss = 17.559\n",
      "iteration 20000: training loss = 14.135 validation loss = 19.656\n",
      "iteration 20100: training loss = 13.971 validation loss = 17.729\n",
      "iteration 20200: training loss = 12.187 validation loss = 17.755\n",
      "iteration 20300: training loss = 13.459 validation loss = 17.854\n",
      "iteration 20400: training loss = 14.048 validation loss = 20.053\n",
      "iteration 20500: training loss = 13.545 validation loss = 18.677\n",
      "iteration 20600: training loss = 13.568 validation loss = 17.823\n",
      "iteration 20700: training loss = 14.389 validation loss = 19.130\n",
      "iteration 20800: training loss = 14.760 validation loss = 18.746\n",
      "iteration 20900: training loss = 14.612 validation loss = 18.435\n",
      "iteration 21000: training loss = 14.155 validation loss = 18.890\n",
      "iteration 21100: training loss = 14.170 validation loss = 19.117\n",
      "iteration 21200: training loss = 12.582 validation loss = 17.383\n",
      "iteration 21300: training loss = 12.818 validation loss = 17.652\n",
      "iteration 21400: training loss = 15.517 validation loss = 19.220\n",
      "iteration 21500: training loss = 15.029 validation loss = 17.622\n",
      "iteration 21600: training loss = 13.519 validation loss = 17.295\n",
      "iteration 21700: training loss = 17.743 validation loss = 20.524\n",
      "iteration 21800: training loss = 12.464 validation loss = 17.705\n",
      "iteration 21900: training loss = 13.192 validation loss = 17.569\n",
      "iteration 22000: training loss = 15.081 validation loss = 19.364\n",
      "iteration 22100: training loss = 12.962 validation loss = 17.557\n",
      "iteration 22200: training loss = 13.586 validation loss = 17.726\n",
      "iteration 22300: training loss = 13.821 validation loss = 17.955\n",
      "iteration 22400: training loss = 12.446 validation loss = 17.125\n",
      "iteration 22500: training loss = 13.604 validation loss = 18.454\n",
      "iteration 22600: training loss = 12.672 validation loss = 17.495\n",
      "iteration 22700: training loss = 13.466 validation loss = 18.265\n",
      "iteration 22800: training loss = 13.436 validation loss = 17.733\n",
      "iteration 22900: training loss = 13.373 validation loss = 17.641\n",
      "iteration 23000: training loss = 13.836 validation loss = 20.564\n",
      "iteration 23100: training loss = 12.946 validation loss = 17.495\n",
      "iteration 23200: training loss = 14.867 validation loss = 18.718\n",
      "iteration 23300: training loss = 14.705 validation loss = 18.826\n",
      "iteration 23400: training loss = 14.632 validation loss = 18.914\n",
      "iteration 23500: training loss = 11.743 validation loss = 17.365\n",
      "iteration 23600: training loss = 13.116 validation loss = 17.910\n",
      "iteration 23700: training loss = 12.607 validation loss = 18.011\n",
      "iteration 23800: training loss = 13.022 validation loss = 17.746\n",
      "iteration 23900: training loss = 13.656 validation loss = 17.904\n",
      "iteration 24000: training loss = 13.480 validation loss = 17.295\n",
      "iteration 24100: training loss = 14.365 validation loss = 17.639\n",
      "iteration 24200: training loss = 13.192 validation loss = 17.190\n",
      "iteration 24300: training loss = 13.004 validation loss = 18.401\n",
      "iteration 24400: training loss = 12.302 validation loss = 17.072\n",
      "iteration 24500: training loss = 11.963 validation loss = 19.046\n",
      "iteration 24600: training loss = 12.175 validation loss = 18.002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 24700: training loss = 12.900 validation loss = 17.596\n",
      "iteration 24800: training loss = 13.475 validation loss = 18.173\n",
      "iteration 24900: training loss = 14.369 validation loss = 17.804\n",
      "iteration 25000: training loss = 13.646 validation loss = 17.345\n",
      "iteration 25100: training loss = 12.356 validation loss = 16.801\n",
      "iteration 25200: training loss = 13.037 validation loss = 16.947\n",
      "iteration 25300: training loss = 12.638 validation loss = 17.130\n",
      "iteration 25400: training loss = 12.287 validation loss = 17.273\n",
      "iteration 25500: training loss = 12.818 validation loss = 17.459\n",
      "iteration 25600: training loss = 12.491 validation loss = 17.460\n",
      "iteration 25700: training loss = 13.008 validation loss = 17.968\n",
      "iteration 25800: training loss = 11.901 validation loss = 17.214\n",
      "iteration 25900: training loss = 11.783 validation loss = 17.100\n",
      "iteration 26000: training loss = 12.247 validation loss = 17.192\n",
      "iteration 26100: training loss = 13.071 validation loss = 17.285\n",
      "iteration 26200: training loss = 12.477 validation loss = 17.013\n",
      "iteration 26300: training loss = 12.877 validation loss = 18.775\n",
      "iteration 26400: training loss = 14.932 validation loss = 19.601\n",
      "iteration 26500: training loss = 12.204 validation loss = 17.292\n",
      "iteration 26600: training loss = 11.802 validation loss = 16.647\n",
      "iteration 26700: training loss = 12.192 validation loss = 18.059\n",
      "iteration 26800: training loss = 13.579 validation loss = 18.525\n",
      "iteration 26900: training loss = 12.146 validation loss = 17.661\n",
      "iteration 27000: training loss = 11.842 validation loss = 16.904\n",
      "iteration 27100: training loss = 11.740 validation loss = 16.854\n",
      "iteration 27200: training loss = 12.686 validation loss = 16.782\n",
      "iteration 27300: training loss = 11.869 validation loss = 16.903\n",
      "iteration 27400: training loss = 11.550 validation loss = 16.096\n",
      "iteration 27500: training loss = 12.075 validation loss = 17.604\n",
      "iteration 27600: training loss = 10.960 validation loss = 16.151\n",
      "iteration 27700: training loss = 12.781 validation loss = 18.484\n",
      "iteration 27800: training loss = 12.457 validation loss = 16.843\n",
      "iteration 27900: training loss = 11.780 validation loss = 15.993\n",
      "iteration 28000: training loss = 11.684 validation loss = 16.348\n",
      "iteration 28100: training loss = 11.112 validation loss = 16.672\n",
      "iteration 28200: training loss = 12.516 validation loss = 17.616\n",
      "iteration 28300: training loss = 12.194 validation loss = 17.883\n",
      "iteration 28400: training loss = 12.648 validation loss = 17.833\n",
      "iteration 28500: training loss = 12.564 validation loss = 17.459\n",
      "iteration 28600: training loss = 11.281 validation loss = 17.296\n",
      "iteration 28700: training loss = 14.148 validation loss = 18.161\n",
      "iteration 28800: training loss = 12.202 validation loss = 16.931\n",
      "iteration 28900: training loss = 12.490 validation loss = 18.655\n",
      "iteration 29000: training loss = 12.998 validation loss = 17.662\n",
      "iteration 29100: training loss = 11.441 validation loss = 16.419\n",
      "iteration 29200: training loss = 12.060 validation loss = 16.447\n",
      "iteration 29300: training loss = 13.190 validation loss = 16.589\n",
      "iteration 29400: training loss = 13.337 validation loss = 17.690\n",
      "iteration 29500: training loss = 11.800 validation loss = 16.814\n",
      "iteration 29600: training loss = 11.336 validation loss = 16.667\n",
      "iteration 29700: training loss = 11.697 validation loss = 16.319\n",
      "iteration 29800: training loss = 11.225 validation loss = 17.294\n",
      "iteration 29900: training loss = 12.452 validation loss = 17.068\n",
      "Total weights for  1100  neurons:  12228801\n",
      "Time to train:  3809.8715257000003\n",
      "training loss to be stored:  12.451605796813965\n",
      "validation loss to be stored:  17.06831932067871\n",
      "6\n",
      "Deleting variables, and going to next neuron size\n"
     ]
    }
   ],
   "source": [
    "training_validation_DF = training.train_on_multiple_neuron_range(labels, spectra,lower_bound,upper_bound,neuron_step_size,steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Number of Neurons</th>\n",
       "      <th>Number of Weights and Biases</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Time to Train(Seconds)</th>\n",
       "      <th>Time to Train(Minutes)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100</td>\n",
       "      <td>1020801</td>\n",
       "      <td>75.922340</td>\n",
       "      <td>75.927040</td>\n",
       "      <td>1251.567760</td>\n",
       "      <td>20.859463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>300</td>\n",
       "      <td>3102401</td>\n",
       "      <td>35.089249</td>\n",
       "      <td>37.557964</td>\n",
       "      <td>1677.157314</td>\n",
       "      <td>27.952622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>500</td>\n",
       "      <td>5264001</td>\n",
       "      <td>23.630301</td>\n",
       "      <td>25.611235</td>\n",
       "      <td>2080.693240</td>\n",
       "      <td>34.678221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>700</td>\n",
       "      <td>7505601</td>\n",
       "      <td>18.030687</td>\n",
       "      <td>21.324554</td>\n",
       "      <td>2936.573770</td>\n",
       "      <td>48.942896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>900</td>\n",
       "      <td>9827201</td>\n",
       "      <td>14.827182</td>\n",
       "      <td>19.799356</td>\n",
       "      <td>3633.286718</td>\n",
       "      <td>60.554779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1100</td>\n",
       "      <td>12228801</td>\n",
       "      <td>12.451606</td>\n",
       "      <td>17.068319</td>\n",
       "      <td>3809.871526</td>\n",
       "      <td>63.497859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Number of Neurons  Number of Weights and Biases  Training Loss  \\\n",
       "0                100                       1020801      75.922340   \n",
       "1                300                       3102401      35.089249   \n",
       "2                500                       5264001      23.630301   \n",
       "3                700                       7505601      18.030687   \n",
       "4                900                       9827201      14.827182   \n",
       "5               1100                      12228801      12.451606   \n",
       "\n",
       "   Validation Loss  Time to Train(Seconds)  Time to Train(Minutes)  \n",
       "0        75.927040             1251.567760               20.859463  \n",
       "1        37.557964             1677.157314               27.952622  \n",
       "2        25.611235             2080.693240               34.678221  \n",
       "3        21.324554             2936.573770               48.942896  \n",
       "4        19.799356             3633.286718               60.554779  \n",
       "5        17.068319             3809.871526               63.497859  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_validation_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_validation_DF.to_pickle(\"training_validation_DF.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = training_validation_DF['Number of Neurons']\n",
    "training_loss = training_validation_DF['Training Loss']\n",
    "validation_loss = training_validation_DF['Validation Loss']\n",
    "time_to_train_mins = training_validation_DF['Time to Train(Minutes)']\n",
    "time_to_train_seconds = training_validation_DF['Time to Train(Seconds)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_weights = training_validation_DF['Number of Weights and Biases']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_axis = x_axis.to_numpy()\n",
    "training_loss = training_loss.to_numpy()\n",
    "validation_loss = validation_loss.to_numpy()\n",
    "time_to_train_mins = time_to_train_mins.to_numpy()\n",
    "time_to_train_seconds = time_to_train_seconds.to_numpy()\n",
    "number_of_weights = number_of_weights.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x720 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axarr = plt.subplots(2,1,figsize=(10,10))\n",
    "\n",
    "ax = axarr[0]\n",
    "ax.plot(number_of_weights,training_loss, ls = '-', color = 'b', label = 'Training Loss')\n",
    "ax.plot(number_of_weights,validation_loss, ls = '-', color = 'r', label = 'Validation Loss Loss')\n",
    "ax.set_yscale('log')\n",
    "# ax.set_xscale('log')\n",
    "# ax.set_ylim([5,100])\n",
    "ax.set_ylabel('Loss', fontsize=20)\n",
    "ax.set_xlabel('Number of Weights and Biases', fontsize=20)\n",
    "\n",
    "\n",
    "ax.legend(ncol=2)\n",
    "\n",
    "ax = axarr[1]\n",
    "ax.plot(number_of_weights,time_to_train_mins, ls = '-', color = 'b', label = 'Time to Train(mins)')\n",
    "# ax.set_yscale('log')\n",
    "# ax.set_ylim([5,100])\n",
    "ax.set_ylabel('Time(minutes)', fontsize=20)\n",
    "ax.set_xlabel('Number of Weights and Biases', fontsize=20)\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "fig.savefig('training_validation_time_to_train' + \".png\",dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
